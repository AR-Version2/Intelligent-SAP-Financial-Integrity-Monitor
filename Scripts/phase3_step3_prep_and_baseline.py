# -*- coding: utf-8 -*-
"""phase3_step3_prep_and_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EKQxhtje5AW5tTd8OrUq31dyV5BGAMui
"""

# --- IMPORTS ---
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
# >>>>> THIS LINE IS CRUCIAL - MAKE SURE IT IS PRESENT AND NOT COMMENTED <<<<<
from sklearn.neighbors import LocalOutlierFactor
# >>>>> ------------------------------------------------------------ <<<<<
import joblib # For saving the scaler
import traceback
import os
import datetime

# --- Configuration ---
# Input File (Engineered Features from Phase 2)
input_directory = r"C:\Users\anith\Downloads"
input_filename = "sap_engineered_features__20250428_205234.csv" # ** CONFIRM THIS FILENAME **
input_file_path = os.path.join(input_directory, input_filename)

# Output Directory (Same as input for this script)
output_directory = r"C:\Users\anith\Downloads"

# --- Script and Output File Naming ---
PHASE_NUMBER = 3
STEP_PREP = 3
STEP_MODEL = "4a" # Using string for substep clarity
TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

# Define output filenames using the new convention
scaler_filename = f"phase{PHASE_NUMBER}_step{STEP_PREP}_scaler_{TIMESTAMP}.pkl"
results_filename = f"phase{PHASE_NUMBER}_step{STEP_MODEL}_baseline_results_{TIMESTAMP}.csv" # Results belong to 4a
identifiers_filename = f"phase{PHASE_NUMBER}_step{STEP_PREP}_identifiers_{TIMESTAMP}.csv"
num_features_filename = f"phase{PHASE_NUMBER}_step{STEP_PREP}_num_features_{TIMESTAMP}.txt"
scaled_features_filename = f"phase{PHASE_NUMBER}_step{STEP_PREP}_scaled_features_{TIMESTAMP}.npy"
SAVE_SCALED_FEATURES_ARRAY = True # Set to True to save the .npy file

# --- Feature Engineering Parameters (Copied for context) ---
PLACEHOLDER_VALUES = ['', '0000000000', '0', '/','__NaN__', None, '#', '000000']

# --- Model Parameters ---
IF_CONTAMINATION = 0.01 # Expect 1% anomalies
LOF_N_NEIGHBORS = 30
LOF_CONTAMINATION = 0.01 # Expect 1% anomalies

# --- Helper Functions ---
def print_separator(title=""): print("\n" + "="*30 + f" {title} " + "="*30)
def safe_to_datetime(series, **kwargs): return pd.to_datetime(series, errors='coerce', **kwargs)
def safe_to_numeric(series, **kwargs): return pd.to_numeric(series, errors='coerce', **kwargs)
def safe_to_string(series):
    if series is None: return None
    if pd.api.types.is_numeric_dtype(series):
        try: return series.astype(float).astype('Int64').astype(str)
        except (ValueError, TypeError): return series.astype(str)
    elif not pd.api.types.is_string_dtype(series): return series.astype(str)
    return series

# --- 1. Load Engineered Data ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_PREP}] Loading Engineered Data: {input_filename}")
df_engineered = None
try:
    df_engineered = pd.read_csv(input_file_path, low_memory=False)
    print(f"Loaded engineered data: {df_engineered.shape}")
except FileNotFoundError: print(f"CRITICAL ERROR: Input file not found at {input_file_path}"); exit()
except Exception as e: print(f"CRITICAL ERROR loading data: {e}"); traceback.print_exc(); exit()

# --- 2. Data Preparation ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_PREP}] Preparing Data for Modeling")

# 2a. Minimal Type Conversion
print("Applying minimal type conversions (identifiers)...")
identifier_cols = ['bukrs', 'belnr', 'gjahr', 'buzei']
for col in identifier_cols:
    if col in df_engineered.columns: df_engineered[col] = safe_to_string(df_engineered[col])
    else: print(f"Warning: Identifier column '{col}' not found.")

# 2b. Handle Missing Values
print("Handling Missing Values...")
cols_to_impute = ['FE_LogAmount', 'FE_AbsoluteAmount']
for col in cols_to_impute:
    if col in df_engineered.columns:
        if df_engineered[col].isnull().any():
            median_val = df_engineered[col].median()
            df_engineered[col] = df_engineered[col].fillna(median_val) # Use assignment
            print(f"  Imputed NaNs in '{col}' with median ({median_val:.4f}).")
feature_columns_all = [col for col in df_engineered.columns if col.startswith('FE_')]
nan_check = df_engineered[feature_columns_all].isnull().sum()
if nan_check.sum() > 0: print(f"ERROR: Unexpected NaNs remain: \n{nan_check[nan_check > 0]}. Stopping."); exit()
else: print("Confirmed no NaNs in feature columns after imputation.")

# 2c. Identify Feature Sets
print("Identifying feature sets...")
numerical_features = [
    'FE_AbsoluteAmount', 'FE_LogAmount',
    'FE_UserPostingFrequency', 'FE_UserAvgLogAmount', 'FE_AmountDeviationFromUserMean',
    'FE_AccountPostingFrequency', 'FE_AccountAvgLogAmount', 'FE_AmountDeviationFromAccountMean',
    'FE_DocTypeFrequency', 'FE_TCodeFrequency'
]
numerical_features = [f for f in numerical_features if f in df_engineered.columns]
print(f"  Numerical features for scaling ({len(numerical_features)}): {numerical_features}")
if not numerical_features: print("ERROR: No numerical features identified. Stopping."); exit()

# 2d. Separate Identifiers
print("Separating identifiers...")
df_identifiers = df_engineered[identifier_cols].reset_index().rename(columns={'index': 'original_index'}).copy()
print(f"  Identifiers shape: {df_identifiers.shape}")

# --- 3. Scale Numerical Features ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_PREP}] Scaling Numerical Features")
scaler = StandardScaler()
print(f"Fitting StandardScaler on {len(numerical_features)} features...")
numerical_features_present = [f for f in numerical_features if f in df_engineered.columns] # Re-check
scaled_features_array = scaler.fit_transform(df_engineered[numerical_features_present])
print(f"  Scaling complete. Scaled data shape: {scaled_features_array.shape}")

# --- 4. Save Scaler & Processed Data (Outputs of Step 3) ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_PREP}] Saving Scaler and Processed Data")
try:
    scaler_path = os.path.join(output_directory, scaler_filename); joblib.dump(scaler, scaler_path); print(f"  Scaler object saved to: {scaler_path}")
    if SAVE_SCALED_FEATURES_ARRAY: scaled_features_path = os.path.join(output_directory, scaled_features_filename); np.save(scaled_features_path, scaled_features_array); print(f"  Scaled features array saved to: {scaled_features_path}")
    num_features_path = os.path.join(output_directory, num_features_filename);
    with open(num_features_path, 'w') as f: f.write('\n'.join(numerical_features_present)); print(f"  Numerical feature names saved to: {num_features_path}")
    identifiers_path = os.path.join(output_directory, identifiers_filename); df_identifiers.to_csv(identifiers_path, index=False, encoding='utf-8'); print(f"  Identifiers saved to: {identifiers_path}")
except Exception as e: print(f"ERROR: Failed to save processed data/scaler: {e}"); traceback.print_exc(); exit()

# --- 5. Baseline Model Training & Prediction (Step 4a) ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_MODEL}] Training Baseline Models")

X_model = scaled_features_array
if_labels = None; if_scores = None; lof_labels = None; lof_scores = None;

# 5a. Isolation Forest
print("\nTraining Isolation Forest...")
if_model = IsolationForest(contamination=IF_CONTAMINATION, random_state=42, n_jobs=-1)
try:
    if_labels = if_model.fit_predict(X_model); if_scores = if_model.decision_function(X_model)
    print(f"  Isolation Forest finished. Found {np.sum(if_labels == -1)} anomalies ({IF_CONTAMINATION:.1%}).")
except Exception as e: print(f"ERROR: Isolation Forest failed: {e}"); traceback.print_exc()

# 5b. Local Outlier Factor
print("\nTraining Local Outlier Factor...")
# *** LOF uses the imported class ***
lof_model = LocalOutlierFactor(n_neighbors=LOF_N_NEIGHBORS, contamination=LOF_CONTAMINATION, n_jobs=-1)
try:
    lof_labels = lof_model.fit_predict(X_model); lof_scores = lof_model.negative_outlier_factor_
    print(f"  Local Outlier Factor finished. Found {np.sum(lof_labels == -1)} anomalies ({LOF_CONTAMINATION:.1%}).")
except Exception as e: print(f"ERROR: Local Outlier Factor failed: {e}"); traceback.print_exc()

# --- 6. Combine and Save Results (Output of Step 4a) ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_MODEL}] Saving Baseline Model Results")

if (if_labels is not None) or (lof_labels is not None): # Check if at least one model ran
    df_results = df_identifiers.copy()
    if if_labels is not None: df_results['IF_Label'] = if_labels
    if if_scores is not None: df_results['IF_Score'] = if_scores
    if lof_labels is not None: df_results['LOF_Label'] = lof_labels
    if lof_scores is not None: df_results['LOF_Score'] = lof_scores

    print(f"\nResults DataFrame head:\n{df_results.head()}")
    print(f"Results DataFrame shape: {df_results.shape}")
    results_path = os.path.join(output_directory, results_filename)
    try:
        df_results.to_csv(results_path, index=False, encoding='utf-8')
        print(f"\nSuccessfully saved baseline model results to: {results_path}")
    except Exception as e: print(f"ERROR: Failed to save results: {e}"); traceback.print_exc()
else:
    print("\nSkipping saving results as baseline models failed or produced no output.")

# --- End of Script ---
print(f"\n--- Script phase3_step3_prep_and_baseline.py ({TIMESTAMP}) Complete ---")
print("Outputs generated:")
print(f" - {scaler_filename}"); print(f" - {identifiers_filename}"); print(f" - {num_features_filename}")
if SAVE_SCALED_FEATURES_ARRAY: print(f" - {scaled_features_filename}")
if os.path.exists(os.path.join(output_directory, results_filename)): print(f" - {results_filename}")