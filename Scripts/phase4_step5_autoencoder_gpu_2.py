# -*- coding: utf-8 -*-
"""phase4_step5_autoencoder_gpu_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D1A56mthB1iNoPsXO53tlee5oT753Rua
"""

# prompt: mount google dricve

from google.colab import drive
drive.mount('/content/drive')

# --- IMPORTS ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, losses, models, callbacks
import joblib
import traceback
import os
import datetime

# --- Colab Specific - Mount Google Drive ---
# from google.colab import drive
# drive.mount('/content/drive')

# --- Configuration ---
drive_base_path = "/content/drive/MyDrive/SAP Financial Doc Anomaly Detection"
input_directory = drive_base_path
output_directory = os.path.join(drive_base_path, f"Phase3_AE_Output_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}")
PLOT_TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d_%H%M%S") # Timestamp for plot filenames and folder
PLOT_OUTPUT_DIR = os.path.join(drive_base_path, f"Phase4_AE_Output_{PLOT_TIMESTAMP}") # Renamed output folder

# --- Input Filenames ---
PREVIOUS_TIMESTAMP_STEP3 = "20250428_230602"
PREVIOUS_TIMESTAMP_STEP4A = "20250428_230602"
engineered_features_timestamp = "20250428_205234"
engineered_features_filename = f"sap_engineered_features__{engineered_features_timestamp}.csv"
scaled_features_filename = f"phase3_step3_scaled_features_{PREVIOUS_TIMESTAMP_STEP3}.npy"
identifiers_filename = f"phase3_step3_identifiers_{PREVIOUS_TIMESTAMP_STEP3}.csv"
num_features_filename = f"phase3_step3_num_features_{PREVIOUS_TIMESTAMP_STEP3}.txt"
results_filename = f"phase3_step4a_baseline_results_{PREVIOUS_TIMESTAMP_STEP4A}.csv" # Needed only for TRAIN_DATA_DEF
input_engineered_path = os.path.join(input_directory, engineered_features_filename)
scaled_features_path = os.path.join(input_directory, scaled_features_filename)
identifiers_path = os.path.join(input_directory, identifiers_filename)
num_features_path = os.path.join(input_directory, num_features_filename)
results_path = os.path.join(input_directory, results_filename) # Baseline results path

# --- Autoencoder Parameters ---
ENCODING_DIM = 4; INTERMEDIATE_DIM = 8; EPOCHS = 50; BATCH_SIZE = 256; VALIDATION_SPLIT = 0.2
TRAINING_DATA_DEF = 'both_normal' # Train only on data flagged as normal by BOTH IF and LOF
anomaly_threshold_percentile = 99 # Flag top 1% by MSE as anomalies

# --- Visualization Parameters ---
TSNE_SAMPLE_SIZE = 50000; TSNE_PERPLEXITY = 30; PCA_N_COMPONENTS = 2; TSNE_N_COMPONENTS = 2
SAVE_PLOTS = True; sns.set_style("whitegrid"); plt.rcParams['figure.figsize'] = (12, 7)

# --- Plotting Configuration ---
PLOT_OUTPUT_DIR = output_directory # Save plots here

# --- Output File Naming ---
SCRIPT_NAME = "autoencoder_gpu"; PHASE_NUMBER = 4; STEP_AE = "5"
TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d_%H%M%S") # Re-get timestamp for outputs of this script run
ae_results_filename = f"phase{PHASE_NUMBER}_step{STEP_AE}_{SCRIPT_NAME}_results_{TIMESTAMP}.csv"

#ae_model_filename = f"phase{PHASE_NUMBER}_step{STEP_AE}_{SCRIPT_NAME}_model_{TIMESTAMP}.h5"

ae_model_filename = f"phase{PHASE_NUMBER}_step{STEP_AE}_{SCRIPT_NAME}_model_{TIMESTAMP}.keras"


# Add filename for AE anomaly profile subset if saving
ae_anomaly_profile_filename = f"phase{PHASE_NUMBER}_step{STEP_AE}_{SCRIPT_NAME}_anomalies_profiled_{TIMESTAMP}.csv"
SAVE_ANOMALY_SUBSETS = True # Flag to save AE anomaly subset

# --- Helper Functions ---
def print_separator(title=""): print("\n" + "="*30 + f" {title} " + "="*30)
def save_plot(plt_obj, filename_base, output_dir):
    if SAVE_PLOTS:
        try:
            filename = f"{filename_base}_{TIMESTAMP}.png"; path = os.path.join(output_dir, filename)
            if not os.path.exists(output_dir): os.makedirs(output_dir); print(f"   * Created plot directory: {output_dir}")
            plt_obj.savefig(path, bbox_inches='tight', dpi=150); print(f"   * Plot saved: {filename}")
        except Exception as e: print(f"   * WARNING: Could not save plot {filename_base}. Error: {e}")
    plt_obj.close()

def compare_feature_stats(df, label_col, feature_list, compare_type='numerical'):
    """ Compares stats (mean/median or value_counts) for anomalies vs normal """
    if label_col not in df.columns: print(f"ERROR: Label column '{label_col}' not found."); return
    anomalies = df[df[label_col] == -1]; normal = df[df[label_col] == 1]
    print(f"\n--- Comparing Features for {label_col} (Anomalies: {len(anomalies)}, Normal: {len(normal)}) ---")
    if len(anomalies) == 0: print("No anomalies found."); return
    comparison_data = []
    for feature in feature_list:
        if feature not in df.columns: continue
        if compare_type == 'numerical':
            mean_anomaly = anomalies[feature].mean(); median_anomaly = anomalies[feature].median()
            mean_normal = normal[feature].mean(); median_normal = normal[feature].median()
            comparison_data.append({'Feature': feature,'Mean (Anomaly)': mean_anomaly,'Median (Anomaly)': median_anomaly,'Mean (Normal)': mean_normal,'Median (Normal)': median_normal})
        elif compare_type == 'binary_flag':
            prop_anomaly = anomalies[feature].mean(); prop_normal = normal[feature].mean()
            comparison_data.append({'Feature': feature,'Prop=1 (Anomaly)': prop_anomaly,'Prop=1 (Normal)': prop_normal})
        elif compare_type == 'categorical':
            print(f"\n  Distribution for '{feature}':"); print("    Anomalies (Top 3):"); print(anomalies[feature].fillna('__NaN__').value_counts(normalize=True).head(3).to_string()); print("    Normal (Top 3):"); print(normal[feature].fillna('__NaN__').value_counts(normalize=True).head(3).to_string())
    if compare_type == 'numerical' or compare_type == 'binary_flag':
        comparison_df = pd.DataFrame(comparison_data)
        if not comparison_df.empty:
             for col in comparison_df.select_dtypes(include=np.number).columns: comparison_df[col] = comparison_df[col].map('{:,.4f}'.format)
             print(comparison_df.to_string(index=False))
        else: print(f"No {compare_type} features found/compared.")

# --- 1. Load Prepared Data ---
# ... (Loading logic remains the same) ...
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Loading Data")
scaled_features_array = None; df_identifiers = None; df_results = None; numerical_feature_names = None; df_engineered_viz = None
try:
    print(f"Loading scaled features from: {scaled_features_path}"); scaled_features_array = np.load(scaled_features_path); print(f"  Loaded scaled features: {scaled_features_array.shape}")
    print(f"Loading identifiers from: {identifiers_path}"); df_identifiers = pd.read_csv(identifiers_path); print(f"  Loaded identifiers: {df_identifiers.shape}")
    print(f"Loading baseline results from: {results_path}"); df_results = pd.read_csv(results_path); print(f"  Loaded results: {df_results.shape}")
    print(f"Loading numerical feature names from: {num_features_path}");
    with open(num_features_path, 'r') as f: numerical_feature_names = [line.strip() for line in f.readlines()]; print(f"  Loaded {len(numerical_feature_names)} numerical feature names.")
    print(f"Loading original engineered features from: {input_engineered_path}"); df_engineered_viz = pd.read_csv(input_engineered_path, low_memory=False); print(f"  Loaded original engineered features: {df_engineered_viz.shape}")
except FileNotFoundError as fnf_error: print(f"CRITICAL ERROR: Input file not found: {fnf_error}. Check paths/timestamps."); exit()
except Exception as e: print(f"CRITICAL ERROR loading data: {e}"); traceback.print_exc(); exit()


# --- 2. Data Preparation for Autoencoder ---
# ... (Prep logic remains the same) ...
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Preparing Data for Autoencoder")
if not (scaled_features_array.shape[0] == len(df_identifiers) == len(df_results) == len(df_engineered_viz)): print("CRITICAL ERROR: Row counts mismatch."); exit()
if not (scaled_features_array.shape[1] == len(numerical_feature_names)): print("CRITICAL ERROR: Scaled features columns mismatch."); exit()
df_train_prep = pd.merge(df_identifiers, df_results, on='original_index', how='left', suffixes=('', '_res'))
if TRAINING_DATA_DEF == 'all': train_indices = df_train_prep['original_index'].values
elif TRAINING_DATA_DEF == 'if_normal': train_indices = df_train_prep[df_train_prep['IF_Label'] == 1]['original_index'].values
elif TRAINING_DATA_DEF == 'lof_normal': train_indices = df_train_prep[df_train_prep['LOF_Label'] == 1]['original_index'].values
elif TRAINING_DATA_DEF == 'both_normal': train_indices = df_train_prep[(df_train_prep['IF_Label'] == 1) & (df_train_prep['LOF_Label'] == 1)]['original_index'].values
else: train_indices = df_train_prep['original_index'].values; print("Warning: Invalid TRAINING_DATA_DEF. Training on ALL data.")
if len(train_indices) == 0: print("CRITICAL ERROR: No training data selected."); exit()
X_train_full = scaled_features_array[train_indices]
print(f"Selected training data shape: {X_train_full.shape}")
X_train, X_val = train_test_split(X_train_full, test_size=VALIDATION_SPLIT, random_state=42)
print(f"Training data shape: {X_train.shape}"); print(f"Validation data shape: {X_val.shape}")
INPUT_DIM = X_train.shape[1]


# --- 3. Define Autoencoder Model ---
# ... (Model definition remains the same) ...
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Defining Autoencoder Model")
autoencoder = models.Sequential([layers.Input(shape=(INPUT_DIM,)), layers.Dense(INTERMEDIATE_DIM, activation='relu'), layers.Dense(ENCODING_DIM, activation='relu'), layers.Dense(INTERMEDIATE_DIM, activation='relu'), layers.Dense(INPUT_DIM, activation='linear')], name="Autoencoder")
autoencoder.compile(optimizer='adam', loss='mse'); autoencoder.summary()


# --- 4. Train Autoencoder Model ---
# ... (Training logic remains the same) ...
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Training Autoencoder")
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
print(f"Starting training for {EPOCHS} epochs (Batch Size: {BATCH_SIZE})...");
history = autoencoder.fit(X_train, X_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, X_val), callbacks=[early_stopping], shuffle=True, verbose=1)
print("Training complete.")
plt.figure(); plt.plot(history.history['loss'], label='Training Loss'); plt.plot(history.history['val_loss'], label='Validation Loss'); plt.title('Autoencoder Training History'); plt.xlabel('Epoch'); plt.ylabel('MSE Loss'); plt.legend(); save_plot(plt, f"{SCRIPT_NAME}_training_history", PLOT_OUTPUT_DIR)


# --- 5. Predict & Calculate Reconstruction Error ---
# ... (Prediction logic remains the same) ...
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Calculating Reconstruction Errors")
print(f"Predicting reconstructions for all {scaled_features_array.shape[0]} data points..."); reconstructions = autoencoder.predict(scaled_features_array); print("Prediction complete.")
mse = np.mean(np.power(scaled_features_array - reconstructions, 2), axis=1)
df_results_ae = df_identifiers.copy(); df_results_ae['AE_MSE_Error'] = mse


# --- 6. Evaluate Anomalies based on Reconstruction Error ---
# ... (Error distribution plot and thresholding remain the same) ...
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Evaluating Anomalies (Reconstruction Error)")
plt.figure(); sns.histplot(df_results_ae['AE_MSE_Error'], bins=50, kde=True); plt.title('Distribution of AE Reconstruction Errors (MSE)'); plt.xlabel('MSE'); plt.ylabel('Frequency'); plt.yscale('log'); save_plot(plt, f"{SCRIPT_NAME}_error_distribution", PLOT_OUTPUT_DIR)
anomaly_threshold_percentile = 99; error_threshold = np.percentile(df_results_ae['AE_MSE_Error'], anomaly_threshold_percentile); print(f"Using {anomaly_threshold_percentile}th percentile MSE as threshold: {error_threshold:.6f}")
df_results_ae['AE_Label'] = np.where(df_results_ae['AE_MSE_Error'] >= error_threshold, -1, 1); n_anomalies_ae = np.sum(df_results_ae['AE_Label'] == -1); print(f"Identified {n_anomalies_ae} anomalies using AE.")


# --- 7. Prepare for Visualization & Profiling Data ---
# Create the merged dataframe needed for both viz and profiling here
print("\n--- Preparing Data for Visualization and Profiling ---")
df_eval_ae = None # Initialize
profiling_possible = False
if df_engineered_viz is not None and df_results_ae is not None:
    if 'original_index' not in df_results_ae.columns: print("ERROR: 'original_index' not found in AE results.")
    else:
        if 'original_index' not in df_engineered_viz.columns: df_engineered_viz = df_engineered_viz.reset_index().rename(columns={'index':'original_index'})
        if 'original_index' in df_engineered_viz.columns:
             cols_to_merge_from_results = ['original_index', 'AE_Label', 'AE_MSE_Error']; cols_to_merge_from_results = [c for c in cols_to_merge_from_results if c in df_results_ae.columns]
             if 'original_index' in cols_to_merge_from_results:
                  df_results_to_merge = df_results_ae[cols_to_merge_from_results].copy()
                  try:
                       df_eval_ae = pd.merge(df_engineered_viz, df_results_to_merge, on='original_index', how='left')
                       print(f"Successfully merged original features with AE results: {df_eval_ae.shape}")
                       profiling_possible = True
                       # Add PCA results for viz (need to ensure df_identifiers aligns with scaled_features_array index)
                       try:
                            pca = PCA(n_components=PCA_N_COMPONENTS, random_state=42).fit(scaled_features_array)
                            pca_result = pca.transform(scaled_features_array)
                            df_eval_ae['PCA1'] = pca_result[:, 0]
                            df_eval_ae['PCA2'] = pca_result[:, 1]
                            print("  Added PCA components to evaluation dataframe.")
                       except Exception as e: print(f"  Warning: Failed to calculate or add PCA components: {e}")

                  except Exception as e: print(f"ERROR during final merge for profiling/viz: {e}"); traceback.print_exc()
             else: print("ERROR: Cannot select needed columns from AE results for merge.")
        else: print("ERROR: Could not establish 'original_index' in df_engineered_viz for merge.")
else: print("Skipping merge for profiling/viz - required dataframes missing.")

# --- 8. Anomaly Profiling (AE) ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Profiling Autoencoder Anomalies")
if profiling_possible and df_eval_ae is not None:
    # Define Feature Lists (** VERIFY THESE **)
    numerical_features_eval = ['hsl','FE_AbsoluteAmount', 'FE_LogAmount','FE_UserPostingFrequency', 'FE_UserAvgLogAmount', 'FE_AmountDeviationFromUserMean','FE_AccountPostingFrequency', 'FE_AccountAvgLogAmount', 'FE_AmountDeviationFromAccountMean','FE_DocTypeFrequency', 'FE_TCodeFrequency']
    binary_features_eval = [col for col in df_eval_ae.columns if col.startswith('FE_Is')]
    categorical_features_eval = ['blart', 'tcode', 'usnam_bkpf', 'racct', 'drcrk']
    if 'FE_PostingHour' in df_eval_ae.columns: numerical_features_eval.append('FE_PostingHour')
    numerical_features_eval = [f for f in numerical_features_eval if f in df_eval_ae.columns]
    binary_features_eval = [f for f in binary_features_eval if f in df_eval_ae.columns]
    categorical_features_eval = [f for f in categorical_features_eval if f in df_eval_ae.columns]

    # Perform comparisons
    print("\nPerforming Statistical Profiling for AE:")
    compare_feature_stats(df_eval_ae, 'AE_Label', numerical_features_eval, compare_type='numerical')
    compare_feature_stats(df_eval_ae, 'AE_Label', binary_features_eval, compare_type='binary_flag')
    compare_feature_stats(df_eval_ae, 'AE_Label', categorical_features_eval, compare_type='categorical')

    # Optional: Save the anomaly subset with original features
    if SAVE_ANOMALY_SUBSETS:
        ae_anomalies = df_eval_ae[df_eval_ae['AE_Label'] == -1].copy()
        ae_anomaly_path = os.path.join(output_directory, ae_anomaly_profile_filename)
        try:
            ae_anomalies.to_csv(ae_anomaly_path, index=False, encoding='utf-8')
            print(f"\nSuccessfully saved AE anomalies (with features) to: {ae_anomaly_path}")
        except Exception as e:
            print(f"ERROR saving AE anomalies: {e}")

else:
    print("Skipping detailed AE anomaly profiling (merge failed or data missing).")


# --- 9. Optional: Visualization for AE Results ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Generating Visualizations for AE Results")

# 9a. PCA Plot colored by AE Anomaly Score (MSE Error)
if profiling_possible and df_eval_ae is not None and 'PCA1' in df_eval_ae.columns:
    if 'AE_MSE_Error' in df_eval_ae.columns:
         print("Generating PCA plot colored by AE Error...")
         plt.figure(figsize=(11, 8));
         try:
             norm = plt.matplotlib.colors.LogNorm(vmin=df_eval_ae['AE_MSE_Error'].quantile(0.01), vmax=df_eval_ae['AE_MSE_Error'].quantile(0.99))
             scatter = plt.scatter(df_eval_ae['PCA1'], df_eval_ae['PCA2'], c=df_eval_ae['AE_MSE_Error'], cmap='viridis', s=5, alpha=0.5, norm=norm)
             plt.colorbar(scatter, label='AE Reconstruction Error (MSE - Log Scale)')
             if 'AE_Label' in df_eval_ae.columns:
                  anomalies_pca = df_eval_ae[df_eval_ae['AE_Label'] == -1]
                  plt.scatter(anomalies_pca['PCA1'], anomalies_pca['PCA2'], color='red', s=15, label=f'Anomaly (Top {100 - anomaly_threshold_percentile}%)', alpha=0.8, edgecolors='k', linewidth=0.5)
                  plt.legend()
             plt.title('PCA Visualization colored by Autoencoder Reconstruction Error'); plt.xlabel('PCA Component 1'); plt.ylabel('PCA Component 2'); save_plot(plt, f"{SCRIPT_NAME}_pca_error", PLOT_OUTPUT_DIR)
         except Exception as e: print(f"  WARNING: Failed to generate PCA AE error plot. Error: {e}"); plt.close()
    else: print("Skipping PCA plot colored by AE error (AE_MSE_Error column not found).")
else: print("Skipping PCA plot colored by AE error (PCA results not available in df_eval_ae).")

# 9b. Feature distribution plots (already done in profiling, but could repeat/save here if needed)


# --- 10. Save AE Results & Model ---  (Section Renumbered)
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_AE}] Saving Autoencoder Results & Model")
# Save results (identifiers + AE error + AE label)
ae_results_path = os.path.join(output_directory, ae_results_filename)
try:
    # Save only the results part, not the full merged df_eval_ae unless desired
    df_results_ae[['original_index', 'AE_MSE_Error', 'AE_Label']].to_csv(ae_results_path, index=False, encoding='utf-8')
    print(f"Successfully saved autoencoder results (ID, Error, Label) to: {ae_results_path}")
except Exception as e: print(f"ERROR saving AE results: {e}")

# Save the trained model
ae_model_path = os.path.join(output_directory, ae_model_filename)
try:
    autoencoder.save(ae_model_path)
    print(f"Successfully saved trained autoencoder model to: {ae_model_path}")
except Exception as e: print(f"ERROR saving AE model: {e}")


print(f"\n--- Script {SCRIPT_NAME}.py ({TIMESTAMP}) Complete ---")
print("Outputs generated:")
print(f" - {ae_results_filename}")
print(f" - {ae_model_filename}")
if SAVE_ANOMALY_SUBSETS and profiling_possible : print(f" - {ae_anomaly_profile_filename}")
if SAVE_PLOTS: print(f" - Plots saved in: {PLOT_OUTPUT_DIR}")