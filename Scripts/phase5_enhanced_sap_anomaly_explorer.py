# -*- coding: utf-8 -*-
"""enhanced_sap_anomaly_explorer_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oKGXG78Hkimwzc0GeZmVXUBcyv6T2iX9
"""

# Required imports at the top
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import seaborn as sns
import traceback
import os
import datetime
# Make sure streamlit-aggrid is installed: pip install streamlit-aggrid
try:
    from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode, JsCode
except ImportError:
    st.error("Missing dependency: streamlit-aggrid. Please install it using 'pip install streamlit-aggrid' and restart.")
    st.stop()

# --- Page Configuration ---
st.set_page_config(page_title="SAP Financial Integrity Monitor", layout="wide", initial_sidebar_state="expanded")

# --- Configuration ---
# IMPORTANT: Adjust these paths if your downloads folder is different
output_directory = r"C:\Users\anith\Downloads" # Example path - ADJUST AS NEEDED
plot_output_dir_base = r"C:\Users\anith\Downloads" # Example path - ADJUST AS NEEDED
logo_path = "sap_logo.png" # <--- CHANGE THIS if needed

# --- EXAMPLE BUKRS to Currency Mapping ---
# *** IMPORTANT: Replace this with your actual mapping ***
# Acquire this externally or define based on knowledge of your company codes
bukrs_to_currency_info = {
    'US01': {'code': 'USD', 'symbol': '$'},
    'DE01': {'code': 'EUR', 'symbol': '€'},
    'EU01': {'code': 'EUR', 'symbol': '€'},
    'GB01': {'code': 'GBP', 'symbol': '£'},
    'JP01': {'code': 'JPY', 'symbol': '¥'},
    'USA1': {'code': 'USD', 'symbol': '$'},
    'C001': {'code': 'CAD', 'symbol': '$'}, # Example where symbol ($) might conflict, code (CAD) is clearer
    'C002': {'code': 'CAD', 'symbol': '$'},
    'C003': {'code': 'CAD', 'symbol': '$'},
    'C004': {'code': 'CAD', 'symbol': '$'},
    'C005': {'code': 'CAD', 'symbol': '$'},
    # Add all relevant company codes from your data
}
# --- End Mapping ---


# --- Plotting Configuration ---
PLOT_TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
plot_output_dir = os.path.join(plot_output_dir_base, f"Streamlit_Viz_Output_{PLOT_TIMESTAMP}")
SAVE_PLOTS = True        #Set to True to save plots locally--------<
sns.set_style("whitegrid")

# --- Helper Functions ---
def print_separator(title=""):
    st.markdown("---")
    if title: st.markdown(f"### {title}")

# <<< UPDATED format_large_currency function (uses Code Prefix) >>>
def format_large_currency(value, precision=2, currency_code=''): # Changed symbol parameter to currency_code
    """Formats a number, adding commas, K/M/B/T suffixes, and a currency code prefix."""
    if pd.isna(value) or not isinstance(value, (int, float)):
        return f"{currency_code} N/A" if currency_code else "N/A"
    value = float(value); abs_value = abs(value); sign = "-" if value < 0 else ""
    if abs_value >= 1e12: formatted_value = f"{abs_value / 1e12:,.{precision}f} T"
    elif abs_value >= 1e9: formatted_value = f"{abs_value / 1e9:,.{precision}f} B"
    elif abs_value >= 1e6: formatted_value = f"{abs_value / 1e6:,.{precision}f} M"
    elif abs_value >= 1e3: formatted_value = f"{abs_value / 1e3:,.{precision}f} K"
    else: formatted_value = f"{abs_value:,.{precision}f}"
    prefix = f"{currency_code} " if currency_code else ""
    return f"{prefix}{sign}{formatted_value}"

def save_plotly_fig(fig, filename_base, output_dir):
    if SAVE_PLOTS: pass # Implementation removed for brevity

def map_consensus_to_label(count):
    if pd.isna(count): return "N/A"
    try: count = int(count);
    except (ValueError, TypeError): return "N/A"
    if count >= 3: return "High"
    elif count == 2: return "Medium"
    elif count == 1: return "Low"
    else: return "N/A"

# --- VaR Calculation and Formatting Helpers ---
def calculate_var_by_bukrs(df, priority_filter=1):
    required_cols = ['Priority_Tier', 'hsl_correct_numeric']; bukrs_col = 'bukrs_correct_value' if 'bukrs_correct_value' in df.columns else 'bukrs'
    if not all(col in df.columns for col in required_cols) or bukrs_col not in df.columns: return None
    try:
        df_copy = df.copy(); df_copy['Priority_Tier'] = pd.to_numeric(df_copy['Priority_Tier'], errors='coerce')
        # Filter for Priority 1, ensuring Priority_Tier is numeric first
        anomalies_p = df_copy.loc[df_copy['Priority_Tier'] == priority_filter]
        if anomalies_p.empty: return pd.Series(dtype=float)
        # Ensure HSL column is numeric for sum
        if not pd.api.types.is_numeric_dtype(anomalies_p['hsl_correct_numeric']):
             print("DEBUG: VaR calc failed - 'hsl_correct_numeric' not numeric.")
             return None
        # Group by the correct BUKRS column and sum absolute HSL
        var_grouped = anomalies_p.groupby(bukrs_col)['hsl_correct_numeric'].apply(lambda x: x.abs().sum())
        # Remove groups with zero value after sum
        var_grouped = var_grouped[~np.isclose(var_grouped, 0)]; print(f"DEBUG: VaR by BUKRS (Prio {priority_filter}): {var_grouped}")
        return var_grouped
    except Exception as e: print(f"ERROR calculating VaR: {e}"); traceback.print_exc(); return None

# <<< UPDATED format_var_for_display function >>>
def format_var_for_display(var_series):
    """Formats the grouped VaR Series into a display string with Currency Codes."""
    if var_series is None: return "Calculation Failed*"
    if var_series.empty: return "N/A (Zero Value)"
    var_display_list = []
    global bukrs_to_currency_info # Access the global mapping
    # Ensure index is string for lookup, sort by amount descending
    var_series.index = var_series.index.astype(str)
    var_series_sorted = var_series.sort_values(ascending=False)
    for bukrs_code, amount in var_series_sorted.items():
        # Get currency info (code and symbol) from the mapping
        currency_info = bukrs_to_currency_info.get(bukrs_code, {}) # Default to empty dict if not found
        code = currency_info.get('code', '') # Default to empty string if code not in info
        # Format the amount using the helper function with the currency code
        formatted_amount = format_large_currency(amount, precision=2, currency_code=code)
        var_display_list.append(f"{formatted_amount} ({bukrs_code})") # Add BUKRS code in parentheses
    if not var_display_list: return "N/A (Zero Value)" # Should not happen if var_series wasn't empty
    return " | ".join(var_display_list) # Join entries with a separator
# --- END HELPER FUNCTIONS ---


# --- Main App Logic ---

# --- Title Section ---
col_logo, col_title = st.columns([1, 6])
with col_logo:
    if os.path.exists(logo_path): st.image(logo_path, width=100)
    else: st.caption(" ") # Placeholder if logo not found
with col_title:
    st.markdown("<h1 style='text-align: center; margin-bottom: 0px;'>Intelligent SAP Financial Integrity Monitor</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center; margin-top: 0px;'>Proactively detecting and prioritizing high-risk financial anomalies.</p>", unsafe_allow_html=True)

# --- State Management Initialization ---
if 'data_loaded' not in st.session_state: st.session_state['data_loaded'] = False
if 'df_anomalies' not in st.session_state: st.session_state['df_anomalies'] = None
if 'df_full_engineered' not in st.session_state: st.session_state['df_full_engineered'] = None
if 'selected_anomaly_index' not in st.session_state: st.session_state['selected_anomaly_index'] = None
# Store original df_full temporarily if needed for fallback lookup (optional)
if '_temp_df_full' not in st.session_state: st.session_state['_temp_df_full'] = None
# Store the final merge keys used
if 'final_merge_keys' not in st.session_state: st.session_state['final_merge_keys'] = []


# --- File Upload Section ---
initial_expanded_state = not st.session_state.get('data_loaded', False)
with st.expander("Upload Data Files", expanded=initial_expanded_state):
    uploaded_prioritized_file = st.file_uploader("Upload Prioritized Anomaly List CSV (Required)", type="csv", key="uploader_anomalies")
    uploaded_engineered_file = st.file_uploader("Upload SAP Features CSV (Required for Amounts)", type="csv", key="uploader_features")
    process_button = st.button("Process Uploaded Files", key="process_button")
    if process_button and (uploaded_prioritized_file is None or uploaded_engineered_file is None):
        st.warning("Please upload BOTH the Prioritized Anomaly List and SAP Features files.")

# --- Processing Logic --- (Uses Composite Key Merge, now potentially including buzei)
if process_button and uploaded_prioritized_file is not None and uploaded_engineered_file is not None:
    try:
        with st.spinner('Loading and processing data...'):
            st.session_state['selected_anomaly_index'] = None # Reset selection on new load
            df_anomalies = None # Initialize
            df_full = None # Initialize

            # --- Load Anomalies File ---
            df_anomalies = pd.read_csv(uploaded_prioritized_file); df_anomalies.attrs['name'] = "Anomalies"
            print(f"Processing {df_anomalies.attrs['name']}...")
            numeric_cols_anom = ['Priority_Tier', 'Model_Anomaly_Count']; date_cols = ['budat_bkpf']
            for col in date_cols:
                if col in df_anomalies.columns: df_anomalies[col] = pd.to_datetime(df_anomalies[col], errors='coerce')
            for col in numeric_cols_anom:
                 if col in df_anomalies.columns: df_anomalies[col] = pd.to_numeric(df_anomalies[col], errors='coerce')

            # --- Load Features File ---
            print(f"DEBUG: Reading SAP Features CSV...")
            df_full = pd.read_csv(uploaded_engineered_file, low_memory=False)


            # --- Determine Merge Keys (Conditionally include buzei) ---
            base_merge_keys = ['bukrs', 'belnr', 'gjahr']
            merge_key_cols = base_merge_keys.copy() # Start with base

            # Check if 'buzei' column exists in BOTH dataframes
            buzei_in_anomalies = 'buzei' in df_anomalies.columns
            buzei_in_features = 'buzei' in df_full.columns

            use_buzei_for_merge = False
            if buzei_in_anomalies and buzei_in_features:
                print("DEBUG: 'buzei' found in both files. Adding to merge keys.")
                merge_key_cols.append('buzei')
                use_buzei_for_merge = True
            elif buzei_in_anomalies or buzei_in_features:
                st.warning("Column 'buzei' found in only one input file. Merging will proceed using header-level keys only (bukrs, belnr, gjahr). Line item details might be inaccurate if anomalies are line-item specific.")
            else:
                 print("DEBUG: 'buzei' not found in input files. Merging using header-level keys (bukrs, belnr, gjahr).")
            st.session_state['final_merge_keys'] = merge_key_cols # Store keys used
            print(f"DEBUG: Final merge keys determined: {merge_key_cols}")


            # --- Clean/Standardize Anomaly Keys & Data (using determined keys) ---
            missing_keys_anom = [k for k in merge_key_cols if k not in df_anomalies.columns]
            if missing_keys_anom: st.error(f"Anomaly file missing REQUIRED keys: {missing_keys_anom}"); st.stop()
            try:
                df_anomalies['bukrs'] = df_anomalies['bukrs'].astype(str).str.strip()
                df_anomalies['belnr'] = df_anomalies['belnr'].astype(str).str.strip()
                df_anomalies['gjahr'] = pd.to_numeric(df_anomalies['gjahr'], errors='coerce').astype('Int64')
                if use_buzei_for_merge: # Only process if buzei is used for merge
                    df_anomalies['buzei'] = pd.to_numeric(df_anomalies['buzei'], errors='coerce').astype('Int64')
            except Exception as e: st.error(f"Error standardizing Anomaly keys: {e}"); st.stop()

            initial_len = len(df_anomalies)
            # Drop rows where any of the REQUIRED merge keys are missing
            df_anomalies.dropna(subset=merge_key_cols, inplace=True)
            if len(df_anomalies) < initial_len: st.warning(f"Dropped {initial_len - len(df_anomalies)} Anomaly rows due to missing required key values ({', '.join(merge_key_cols)}).")
            if df_anomalies.empty: st.error("Anomaly file empty after key cleaning."); st.stop()
            # Add original index *after* cleaning
            df_anomalies.reset_index(drop=True, inplace=True); df_anomalies.reset_index(inplace=True); df_anomalies.rename(columns={'index': 'original_index'}, inplace=True)


            # --- Clean/Standardize Feature Keys & Data (using determined keys) ---
            df_full_engineered = None # Initialize indexed features df
            try:
                # Check for essential feature columns needed BEFORE merge (keys + hsl)
                required_feature_cols_pre_merge = merge_key_cols + ['hsl']
                missing_feat_cols = [c for c in required_feature_cols_pre_merge if c not in df_full.columns]
                if missing_feat_cols: st.error(f"Features file missing essential columns for merge: {missing_feat_cols}"); st.stop()

                # Standardize keys and HSL
                try:
                    df_full['bukrs'] = df_full['bukrs'].astype(str).str.strip()
                    df_full['belnr'] = df_full['belnr'].astype(str).str.strip()
                    df_full['gjahr'] = pd.to_numeric(df_full['gjahr'], errors='coerce').astype('Int64')
                    if use_buzei_for_merge: # Only process if buzei is used for merge
                        df_full['buzei'] = pd.to_numeric(df_full['buzei'], errors='coerce').astype('Int64')
                    # Clean HSL column needed for merge and VaR
                    df_full['hsl'] = pd.to_numeric(df_full['hsl'], errors='coerce')
                except Exception as e: st.error(f"Error standardizing Feature keys/HSL: {e}"); st.stop()

                # Drop rows with missing REQUIRED keys or missing HSL
                initial_len_full = len(df_full)
                columns_to_check_na = merge_key_cols + ['hsl']
                df_full.dropna(subset=columns_to_check_na, inplace=True)
                if len(df_full) < initial_len_full: st.warning(f"Dropped {initial_len_full - len(df_full)} Feature rows due to missing required key values ({', '.join(merge_key_cols)}) or missing HSL.")
                if df_full.empty: st.error("Features file empty after cleaning essential columns."); st.stop()

                # Handle duplicate keys in Features (using the final merge_key_cols) - keep first
                if df_full.duplicated(subset=merge_key_cols).any():
                    num_duplicates = df_full.duplicated(subset=merge_key_cols).sum()
                    st.warning(f"Duplicate key combinations ({', '.join(merge_key_cols)}) found in Features file ({num_duplicates} rows). Keeping the first instance for each combination.")
                    df_full.drop_duplicates(subset=merge_key_cols, keep='first', inplace=True)

                # --- Store a copy of the cleaned df_full before indexing (for fallback) ---
                st.session_state['_temp_df_full'] = df_full.copy()

                # --- Attempt to create indexed copy (df_full_engineered) ---
                # (This logic remains the same - tries to use 'original_index' if present)
                df_full_engineered_copy = df_full.copy()
                if 'original_index' in df_full_engineered_copy.columns:
                     df_full_engineered_copy['original_index'] = pd.to_numeric(df_full_engineered_copy['original_index'], errors='coerce')
                     if not df_full_engineered_copy['original_index'].isnull().any() and not df_full_engineered_copy['original_index'].duplicated().any():
                         try:
                             if (df_full_engineered_copy['original_index'] % 1 == 0).all():
                                 try: df_full_engineered_copy['original_index'] = df_full_engineered_copy['original_index'].astype('Int64')
                                 except Exception: pass
                             df_full_engineered_copy.set_index('original_index', inplace=True, drop=False)
                             df_full_engineered = df_full_engineered_copy
                             print("DEBUG: Indexed features copy stored in df_full_engineered.")
                         except Exception as idx_e:
                             print(f"DEBUG: Could not use 'original_index' from features file as index: {idx_e}")
                             df_full_engineered = None
                else:
                     print("DEBUG: 'original_index' column not found in features file. df_full_engineered will not be indexed.")
                     df_full_engineered = None

            except Exception as eng_e: st.error(f"Could not process SAP Features file: {eng_e}."); traceback.print_exc(); st.stop()


            # --- Perform Merge using determined Composite Key ---
            print(f"DEBUG: Merging using keys: {merge_key_cols}...")

            # Select necessary columns from features: keys + hsl
            columns_needed_from_features = merge_key_cols + ['hsl']
            columns_needed_from_features = list(dict.fromkeys(columns_needed_from_features)) # Ensure unique
            print(f"DEBUG: Columns selected from Features for merge: {columns_needed_from_features}")
            features_to_merge = df_full[columns_needed_from_features].copy()

            # Perform the left merge
            df_anomalies_merged = pd.merge(
                df_anomalies,
                features_to_merge,
                on=merge_key_cols, # Use the determined keys
                how='left',
                suffixes=('', '_feature')
            )
            print(f"DEBUG: Merge complete. Shape: {df_anomalies_merged.shape}")

            # --- Handle Merged Columns (Same logic as before) ---
            if 'hsl' in df_anomalies_merged.columns:
                df_anomalies_merged['hsl_correct_value'] = df_anomalies_merged['hsl']
                print("DEBUG: Using 'hsl' column directly from features merge.")
            elif 'hsl_feature' in df_anomalies_merged.columns:
                 df_anomalies_merged['hsl_correct_value'] = df_anomalies_merged['hsl_feature']
                 print("DEBUG: Using 'hsl_feature' (suffixed) column from features merge.")
            else:
                 df_anomalies_merged['hsl_correct_value'] = np.nan
                 st.warning("HSL column ('hsl' or 'hsl_feature') missing after merge. Amounts will be N/A.")

            if 'bukrs' in df_anomalies_merged.columns:
                 df_anomalies_merged['bukrs_correct_value'] = df_anomalies_merged['bukrs'].astype(str)
                 print("DEBUG: Using 'bukrs' column (merge key) as the correct company code.")
            else:
                 df_anomalies_merged['bukrs_correct_value'] = 'UNKNOWN'
                 st.error("CRITICAL: 'bukrs' column missing after merge despite being a merge key.")

            df_anomalies_merged['hsl_correct_numeric'] = pd.to_numeric(df_anomalies_merged['hsl_correct_value'], errors='coerce').fillna(0)
            merge_failures = df_anomalies_merged['hsl_correct_value'].isna().sum()
            if merge_failures > 0:
                st.warning(f"{merge_failures} anomalies couldn't be matched to features using keys [{', '.join(merge_key_cols)}], or the matched feature row had a missing/invalid HSL value.")

            # --- Set Final Index ---
            if 'original_index' in df_anomalies_merged.columns:
                if not df_anomalies_merged['original_index'].duplicated().any():
                    df_anomalies_merged.set_index('original_index', inplace=True, drop=True)
                    print(f"DEBUG: Final merged DataFrame index set to 'original_index'. Name: {df_anomalies_merged.index.name}")
                else:
                    st.error("CRITICAL: Duplicate values found in 'original_index' column after merge. Cannot set index reliably.")
                    st.stop()
            else:
                st.error("CRITICAL: 'original_index' column lost during merge process. Cannot set index.")
                st.stop()

            # Store final processed data in Session State
            st.session_state['df_anomalies'] = df_anomalies_merged
            st.session_state['df_full_engineered'] = df_full_engineered # Might be None
            st.session_state['data_loaded'] = True
            print("--- Data Processing Complete ---"); st.success("Anomaly data loaded successfully!"); st.rerun()

    except ValueError as ve: # Catch specific merge/processing errors
        print(f"--- VALUE ERROR DURING PROCESSING ---")
        traceback.print_exc()
        st.error(f"Processing Error: {ve}")
        st.error("This often indicates an issue with column names, data types during conversion, or the merge operation. Check console DEBUG messages.")
        st.session_state['data_loaded'] = False
    except Exception as e:
        print(f"--- UNEXPECTED ERROR DURING PROCESSING ---")
        traceback.print_exc()
        st.error(f"An unexpected error occurred during processing: {e}")
        st.session_state['data_loaded'] = False


# --- Display Sections ---
if st.session_state.get('data_loaded', False):
    df_anomalies = st.session_state.get('df_anomalies')
    df_full_engineered = st.session_state.get('df_full_engineered') # Might be None
    final_merge_keys = st.session_state.get('final_merge_keys', []) # Get keys used

    # Basic checks after loading from state
    if df_anomalies is None or not isinstance(df_anomalies, pd.DataFrame) or df_anomalies.index.name != 'original_index':
        st.error("Anomaly DataFrame corrupted or index missing in session state. Please re-upload.")
        st.stop()
    if 'hsl_correct_numeric' not in df_anomalies.columns or 'bukrs_correct_value' not in df_anomalies.columns:
        st.warning("Corrected amount ('hsl_correct_numeric') or company code ('bukrs_correct_value') column is missing. VaR and formatting might fail.")
    if df_full_engineered is not None and (not isinstance(df_full_engineered, pd.DataFrame) or df_full_engineered.index.name != 'original_index'):
        st.warning("The indexed 'Features' DataFrame (df_full_engineered) seems corrupted or has lost its index.")


    # --- Total Data Overview --- (Uses st.markdown for VaR)
    print_separator("Dataset Snapshot")
    st.markdown("Summary statistics for the entire uploaded 'Prioritized Anomaly List'.")

    # --- UPDATED VaR DISPLAY ---
    # Use columns for the first two metrics only
    tot_kpi1, tot_kpi2 = st.columns(2)
    total_anomalies_full = len(df_anomalies)
    total_priority1_full = 0
    if 'Priority_Tier' in df_anomalies.columns:
        try:
            priority_numeric = pd.to_numeric(df_anomalies['Priority_Tier'], errors='coerce')
            total_priority1_full = len(df_anomalies[priority_numeric == 1])
        except Exception as e_prio:
            print(f"DEBUG: Error counting Prio1: {e_prio}")

    tot_kpi1.metric(label="Total Anomalies Loaded", value=f"{total_anomalies_full:,}")
    tot_kpi2.metric(label="Total Priority 1 Anomalies", value=f"{total_priority1_full:,}")

    # Display VaR using st.markdown below the columns for better wrapping
    var_by_bukrs_full = calculate_var_by_bukrs(df_anomalies, priority_filter=1)
    value_at_risk_display_full = format_var_for_display(var_by_bukrs_full)
    st.markdown(f"**Priority 1 Value at Risk (by CoCode):**") # Label in bold
    st.markdown(f"{value_at_risk_display_full}") # Value on next line, allows wrapping
    # --- END UPDATED VaR DISPLAY ---

    st.caption("*Value at Risk based on absolute sum of 'hsl' from features file, grouped by Company Code.*") # Caption remains

    # Other KPIs remain in columns
    tot_sum1, tot_sum2, tot_sum3 = st.columns(3)
    with tot_sum1: st.metric("Total Unique Users", f"{df_anomalies['usnam_bkpf'].nunique() if 'usnam_bkpf' in df_anomalies.columns else 0:,}")
    with tot_sum2: st.metric("Total Unique Doc Types", f"{df_anomalies['blart'].nunique() if 'blart' in df_anomalies.columns else 0:,}")
    with tot_sum3: st.metric("Total Unique Trans Codes", f"{df_anomalies['tcode'].nunique() if 'tcode' in df_anomalies.columns else 0:,}")

    # --- Data Date Range ---
    date_range_str = "N/A";
    if 'budat_bkpf' in df_anomalies.columns and pd.api.types.is_datetime64_any_dtype(df_anomalies['budat_bkpf']):
         min_total_date = df_anomalies['budat_bkpf'].min(); max_total_date = df_anomalies['budat_bkpf'].max()
         if pd.notna(min_total_date) and pd.notna(max_total_date): date_range_str = f"{min_total_date.strftime('%d-%b-%Y')} to {max_total_date.strftime('%d-%b-%Y')}"
         elif pd.notna(min_total_date): date_range_str = f"From {min_total_date.strftime('%d-%b-%Y')}"
         elif pd.notna(max_total_date): date_range_str = f"Up to {max_total_date.strftime('%d-%b-%Y')}"
    st.markdown(f"**Data Date Range:** {date_range_str}")

    # --- Anomaly Explorer Section ---
    print_separator("Anomaly Explorer")
    # --- Sidebar Filters ---
    st.sidebar.header("Filters")
    try:
        # Use available columns for filters, handle potential missing columns gracefully
        tier_options = []
        if 'Priority_Tier' in df_anomalies.columns:
             tier_options = sorted(df_anomalies['Priority_Tier'].dropna().unique().astype(int))
        selected_tier = st.sidebar.multiselect("Anomaly Priority", options=tier_options, default=[1] if 1 in tier_options else tier_options)

        consensus_map_options = {"High": [3], "Medium": [2], "Low": [1]}
        available_consensus_labels = []
        selected_consensus_numbers = []
        if 'Model_Anomaly_Count' in df_anomalies.columns:
            available_consensus_labels = sorted(list(set(map_consensus_to_label(c) for c in df_anomalies['Model_Anomaly_Count'].dropna().unique() if map_consensus_to_label(c) != "N/A")))
            default_consensus = [lbl for lbl in ["High", "Medium"] if lbl in available_consensus_labels]; default_consensus = default_consensus if default_consensus else available_consensus_labels
            selected_consensus_labels = st.sidebar.multiselect("Model Consensus", options=available_consensus_labels, default=default_consensus)
            for lbl in selected_consensus_labels: selected_consensus_numbers.extend(consensus_map_options.get(lbl, []))

        user_options = []
        if 'usnam_bkpf' in df_anomalies.columns:
            user_options = sorted(df_anomalies['usnam_bkpf'].dropna().astype(str).unique())
        selected_user = st.sidebar.multiselect("User (usnam_bkpf)", options=user_options, default=[])

        blart_options = []
        if 'blart' in df_anomalies.columns:
            blart_options = sorted(df_anomalies['blart'].dropna().astype(str).unique())
        selected_blart = st.sidebar.multiselect("Document Type (blart)", options=blart_options, default=blart_options)

        tcode_options = []
        if 'tcode' in df_anomalies.columns:
            tcode_options = sorted(df_anomalies['tcode'].dropna().astype(str).unique())
        selected_tcode = st.sidebar.multiselect("Transaction Code (tcode)", options=tcode_options, default=tcode_options)

        # Add buzei filter if it was used in merge
        selected_buzei = None
        if 'buzei' in final_merge_keys and 'buzei' in df_anomalies.columns:
            # Simple text input for specific line item for now
            selected_buzei = st.sidebar.text_input("Filter Line Item (buzei)", key="filter_buzei")


        selected_start_date, selected_end_date = None, None
        if 'budat_bkpf' in df_anomalies.columns and pd.api.types.is_datetime64_any_dtype(df_anomalies['budat_bkpf']):
             min_date_filt = df_anomalies['budat_bkpf'].min(); max_date_filt = df_anomalies['budat_bkpf'].max()
             if pd.notna(min_date_filt) and pd.notna(max_date_filt):
                 default_start = min_date_filt.date(); default_end = max_date_filt.date()
                 min_allowed = min_date_filt.date(); max_allowed = max_date_filt.date()
                 if min_allowed > max_allowed: min_allowed = max_allowed # Handle single date case
                 selected_start_date = st.sidebar.date_input("Start Date Filter", value=default_start, min_value=min_allowed, max_value=max_allowed, key="filter_start_date")
                 selected_end_date = st.sidebar.date_input("End Date Filter", value=default_end, min_value=min_allowed, max_value=max_allowed, key="filter_end_date")

        search_belnr = st.sidebar.text_input("Search Document Number (belnr)")
    except Exception as e: st.sidebar.error(f"Filter setup error: {e}"); selected_tier, selected_consensus_numbers, selected_user, selected_blart, selected_tcode, selected_start_date, selected_end_date, search_belnr, selected_buzei = [], [], [], [], [], None, None, "", None


    # --- Filter Data ---
    filtered_df = df_anomalies.copy() # Start with the full, processed anomaly data
    try:
        # Apply filters sequentially, checking if column exists
        if selected_tier and 'Priority_Tier' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['Priority_Tier'].isin(selected_tier)]
        if selected_consensus_numbers and 'Model_Anomaly_Count' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['Model_Anomaly_Count'].isin(selected_consensus_numbers)]
        if selected_user and 'usnam_bkpf' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['usnam_bkpf'].astype(str).isin([str(u) for u in selected_user])]
        if selected_blart and 'blart' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['blart'].astype(str).isin([str(b) for b in selected_blart])]
        if selected_tcode and 'tcode' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['tcode'].astype(str).isin([str(t) for t in selected_tcode])]

        # BUZEI Filter (if applicable and column exists)
        if selected_buzei and 'buzei' in final_merge_keys and 'buzei' in filtered_df.columns:
            try:
                # Attempt to convert input to numeric for comparison, handle errors
                buzei_numeric_filter = pd.to_numeric(selected_buzei, errors='coerce')
                if pd.notna(buzei_numeric_filter):
                     # Filter where 'buzei' column matches the numeric input
                     # Make sure the column type is compatible (e.g., both int or float)
                     filtered_df = filtered_df[pd.to_numeric(filtered_df['buzei'], errors='coerce') == buzei_numeric_filter]
                elif selected_buzei.strip() != "": # Only warn if input is not empty and not numeric
                     st.sidebar.warning(f"Invalid input for Line Item filter: '{selected_buzei}'. Please enter a number.")
            except Exception as e_buzei_filt:
                st.sidebar.error(f"Error applying Line Item filter: {e_buzei_filt}")


        # Date Filtering
        if selected_start_date and 'budat_bkpf' in filtered_df.columns and pd.api.types.is_datetime64_any_dtype(filtered_df['budat_bkpf']):
            start_date_dt = pd.to_datetime(selected_start_date)
            date_mask_start = filtered_df['budat_bkpf'].notna()
            filtered_df = filtered_df[date_mask_start & (filtered_df['budat_bkpf'] >= start_date_dt)]
        if selected_end_date and 'budat_bkpf' in filtered_df.columns and pd.api.types.is_datetime64_any_dtype(filtered_df['budat_bkpf']):
            end_date_dt = pd.to_datetime(selected_end_date) + pd.Timedelta(days=1) # Include end date
            date_mask_end = filtered_df['budat_bkpf'].notna()
            filtered_df = filtered_df[date_mask_end & (filtered_df['budat_bkpf'] < end_date_dt)]

        # BELNR Search
        if search_belnr and 'belnr' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['belnr'].astype(str).str.contains(search_belnr.strip(), case=False, na=False)]

    except Exception as e:
        st.error(f"Error applying filters: {e}")
        traceback.print_exc()
        filtered_df = pd.DataFrame(index=pd.Index([], name='original_index', dtype=df_anomalies.index.dtype))


    # --- Filtered Data Display (KPIs) --- (Uses st.markdown for VaR)
    st.subheader("Overview (Filtered Data)")

    # --- UPDATED VaR DISPLAY ---
    # Use columns for the first two metrics only
    filt_kpi1, filt_kpi2 = st.columns(2)
    total_anomalies_filtered = len(filtered_df)
    priority1_anomalies_filtered = 0
    if not filtered_df.empty:
        if 'Priority_Tier' in filtered_df.columns:
            priority1_anomalies_filtered = len(filtered_df[pd.to_numeric(filtered_df['Priority_Tier'], errors='coerce') == 1])

    filt_kpi1.metric(label="Total Anomalies Displayed", value=f"{total_anomalies_filtered:,}")
    filt_kpi2.metric(label="Priority 1 Anomalies Displayed", value=f"{priority1_anomalies_filtered:,}")

    # Display VaR using st.markdown below the columns for better wrapping
    var_by_bukrs_filtered = calculate_var_by_bukrs(filtered_df, priority_filter=1)
    value_at_risk_display_filtered = format_var_for_display(var_by_bukrs_filtered)
    st.markdown(f"**Priority 1 Value at Risk (Filtered, by CoCode):**") # Label in bold
    st.markdown(f"{value_at_risk_display_filtered}") # Value on next line, allows wrapping
    # --- END UPDATED VaR DISPLAY ---

    st.markdown("---") # Separator remains

    # --- Visualizations --- (Logic remains largely the same)
    st.subheader("Anomaly Visualizations (Filtered Data)")
    if not filtered_df.empty:
        viz_row1_col1, viz_row1_col2 = st.columns(2)
        with viz_row1_col1: # Top Users
             st.markdown("##### Top 5 Users by Anomaly Count")
             if 'usnam_bkpf' in filtered_df.columns:
                try:
                    user_counts = filtered_df['usnam_bkpf'].dropna().astype(str).value_counts().nlargest(5).reset_index()
                    user_counts.columns = ['usnam_bkpf', 'count']
                    if not user_counts.empty:
                        fig = px.bar(user_counts.sort_values('count', ascending=True), x='count', y='usnam_bkpf', orientation='h', labels={'count': 'Anomalies', 'usnam_bkpf': 'User'}, text='count', height=350)
                        fig.update_traces(textposition='outside'); fig.update_layout(yaxis={'categoryorder':'total ascending'}, margin=dict(l=20, r=20, t=30, b=20))
                        st.plotly_chart(fig, use_container_width=True)
                    else: st.caption("No user data available in the filtered results.")
                except Exception as e: st.error(f"User Plot Error: {e}")
             else: st.caption("'usnam_bkpf' column not found in data.")
        with viz_row1_col2: # Top Doc Types
             st.markdown("##### Top 5 Document Types by Anomaly Count")
             if 'blart' in filtered_df.columns:
                try:
                    blart_counts = filtered_df['blart'].fillna('N/A').astype(str).str.strip(); blart_counts = blart_counts[blart_counts != ''].value_counts().nlargest(5).reset_index(); blart_counts.columns = ['blart', 'count']
                    if not blart_counts.empty:
                        fig = px.bar(blart_counts, x='blart', y='count', labels={'blart': 'Doc Type', 'count': 'Anomalies'}, text_auto=True, height=350)
                        fig.update_layout(xaxis_tickangle=-45, margin=dict(l=20, r=20, t=30, b=20))
                        st.plotly_chart(fig, use_container_width=True)
                    else: st.caption("No document type data available in the filtered results.")
                except Exception as e: st.error(f"Doc Type Plot Error: {e}")
             else: st.caption("'blart' column not found in data.")

        st.markdown("---")
        st.markdown("##### Anomalies Over Time (by Posting Date)")
        if 'budat_bkpf' in filtered_df.columns and pd.api.types.is_datetime64_any_dtype(filtered_df['budat_bkpf']):
            try:
                time_df_source = filtered_df.dropna(subset=['budat_bkpf'])
                if not time_df_source.empty:
                     date_range_days = (time_df_source['budat_bkpf'].max() - time_df_source['budat_bkpf'].min()).days
                     resample_freq = 'ME' if date_range_days > 90 else ('W' if date_range_days > 14 else 'D')
                     temp_time_df = time_df_source.set_index('budat_bkpf'); time_df = temp_time_df.resample(resample_freq).size().reset_index(); time_df.columns = ['Date', 'Number of Anomalies']
                     time_df = time_df[time_df['Number of Anomalies'] > 0] # Remove periods with zero anomalies
                     if not time_df.empty:
                         fig = px.line(time_df, x='Date', y='Number of Anomalies', markers=True, labels={'Date': f'Time ({resample_freq})', 'Number of Anomalies': 'Anomaly Count'})
                         fig.update_layout(margin=dict(l=20, r=20, t=30, b=20)); st.plotly_chart(fig, use_container_width=True)
                     else: st.caption("No anomalies found in the selected time period after resampling.")
                else: st.caption("No valid posting dates found in the filtered data.")
            except Exception as e: st.error(f"Time Series Plot Error: {e}")
        elif 'budat_bkpf' in filtered_df.columns: st.caption("Posting date ('budat_bkpf') column is not in a recognized date format.")
        else: st.caption("Posting date ('budat_bkpf') column not found in data.")

        st.markdown("---")
        st.subheader("High-Risk Flag Frequency (Filtered Data)")
        hrf_cols = [col for col in filtered_df.columns if col.startswith('HRF_')]
        if hrf_cols:
            try:
                hrf_cols_present = [col for col in hrf_cols if col in filtered_df.columns]
                if hrf_cols_present:
                    hrf_numeric_cols = [col for col in hrf_cols_present if pd.api.types.is_numeric_dtype(filtered_df[col].dtype) or pd.api.types.is_bool_dtype(filtered_df[col].dtype)]
                    if hrf_numeric_cols:
                         hrf_summary = filtered_df[hrf_numeric_cols].apply(pd.to_numeric, errors='coerce').sum().reset_index();
                         hrf_summary.columns = ['Risk Factor', 'Count']; hrf_summary = hrf_summary[hrf_summary['Count'] > 0].sort_values('Count', ascending=False)
                         if not hrf_summary.empty:
                             hrf_summary['Risk Factor Readable'] = hrf_summary['Risk Factor'].apply(lambda x: ''.join([' ' + char if char.isupper() else char for char in x.replace('HRF_', '').replace('Is', '')]).strip())
                             fig = px.bar(hrf_summary, x='Risk Factor Readable', y='Count', title='Frequency of High-Risk Flags', labels={'Risk Factor Readable': 'Risk Factor', 'Count':'Frequency'}, text_auto=True)
                             fig.update_layout(xaxis_tickangle=-45, margin=dict(l=20, r=20, t=50, b=20)); st.plotly_chart(fig, use_container_width=True)
                         else: st.caption("No High-Risk Flags were triggered in the filtered data.")
                    else: st.caption("No numeric or boolean HRF columns found to analyze.")
                else: st.caption("Defined HRF columns (starting with 'HRF_') not found in the filtered data.")
            except Exception as e: st.error(f"HRF Plot Error: {e}")
        else: st.caption("No columns starting with 'HRF_' found to analyze.")
    else: # filtered_df is empty
        st.caption("No data matches the current filters. Adjust filters to see visualizations.")

    # --- Anomaly Investigation List (Using AgGrid) ---
    st.markdown("---")
    st.subheader("Anomaly Investigation List (Filtered)")

    if filtered_df.index.name != 'original_index':
        st.error(f"CRITICAL: Filtered DataFrame index lost its name ('original_index'). Cannot display grid.")
        st.stop()

    # Prepare dataframe for AgGrid
    grid_display_df = filtered_df.reset_index()

    # Add calculated/formatted columns needed for display
    if 'Model_Anomaly_Count' in grid_display_df.columns:
        grid_display_df['Model_Consensus_Label'] = grid_display_df['Model_Anomaly_Count'].apply(map_consensus_to_label)
    else: grid_display_df['Model_Consensus_Label'] = "N/A"

    if 'hsl_correct_numeric' in grid_display_df.columns and 'bukrs_correct_value' in grid_display_df.columns:
        grid_display_df['Amount_Formatted'] = grid_display_df.apply(
            lambda row: format_large_currency(
                row['hsl_correct_numeric'],
                currency_code=bukrs_to_currency_info.get(str(row['bukrs_correct_value']), {}).get('code', '')
            ) if pd.notna(row['hsl_correct_numeric']) else 'N/A', axis=1
        )
    else:
        grid_display_df['Amount_Formatted'] = 'N/A (Missing Data)'

    # Display Grid if data exists
    if not grid_display_df.empty:
        try:
            gb = GridOptionsBuilder.from_dataframe(grid_display_df)

            # Define columns to display prominently and their order
            # *** ADD buzei to the list if it was used ***
            cols_to_display_ordered = [
                'belnr', 'bukrs_correct_value', 'gjahr',
                # Conditionally include buzei
                *(['buzei'] if 'buzei' in final_merge_keys else []),
                'Priority_Tier', 'Model_Consensus_Label', 'Amount_Formatted', 'budat_bkpf',
                'usnam_bkpf', 'blart', 'tcode', 'racct', 'Review_Focus',
                'original_index' # Keep index for selection
            ]
            cols_in_grid = [col for col in cols_to_display_ordered if col in grid_display_df.columns]

            # Configure visible columns
            if 'belnr' in cols_in_grid: gb.configure_column("belnr", header_name="Doc No.", width=110, filter='agTextColumnFilter') # Abbrev
            if 'bukrs_correct_value' in cols_in_grid: gb.configure_column("bukrs_correct_value", header_name="CoCode", width=80, filter='agTextColumnFilter') # Abbrev
            if 'gjahr' in cols_in_grid: gb.configure_column("gjahr", header_name="Year", width=70, filter=True) # Abbrev
            # *** Configure buzei if present ***
            if 'buzei' in cols_in_grid: gb.configure_column("buzei", header_name="Line", width=70, filter=True) # Abbrev & Filter
            if 'Priority_Tier' in cols_in_grid: gb.configure_column("Priority_Tier", header_name="Prio", width=60, filter=True) # Abbrev
            if 'Model_Consensus_Label' in cols_in_grid: gb.configure_column("Model_Consensus_Label", header_name="Consensus", width=90, filter='agTextColumnFilter') # Abbrev
            if 'Amount_Formatted' in cols_in_grid: gb.configure_column("Amount_Formatted", header_name="Amount (Local)", width=130, filter='agTextColumnFilter', sortable=True) # Abbrev
            if 'budat_bkpf' in cols_in_grid: gb.configure_column("budat_bkpf", header_name="Post Date", width=100, filter='agDateColumnFilter', type=["dateColumnFilter", "customDateTimeFormat"], custom_format_string='yyyy-MM-dd') # Abbrev
            if 'usnam_bkpf' in cols_in_grid: gb.configure_column("usnam_bkpf", header_name="User Name", width=110, filter='agTextColumnFilter')
            if 'blart' in cols_in_grid: gb.configure_column("blart", header_name="Doc Type", width=90, filter='agTextColumnFilter') # Abbrev
            if 'tcode' in cols_in_grid: gb.configure_column("tcode", header_name="TCode", width=90, filter='agTextColumnFilter') # Abbrev
            if 'racct' in cols_in_grid: gb.configure_column("racct", header_name="Account", width=100, filter='agTextColumnFilter')
            if 'Review_Focus' in cols_in_grid: gb.configure_column("Review_Focus", header_name="Anomaly Reason", width=300, wrapText=True, autoHeight=True)

            # Hide other columns
            all_grid_cols = grid_display_df.columns.tolist(); visible_cols_set = set(cols_in_grid)
            if 'original_index' in visible_cols_set: visible_cols_set.remove('original_index')
            gb.configure_column('original_index', hide=True)
            for col in all_grid_cols:
                if col not in visible_cols_set and col != 'original_index':
                    if col in grid_display_df.columns: gb.configure_column(col, hide=True)

            # General Grid Config
            gb.configure_selection('single', use_checkbox=False)
            gb.configure_pagination(paginationAutoPageSize=False, paginationPageSize=15)
            gb.configure_grid_options(domLayout='normal')
            gb.configure_default_column(sortable=True, filter=True) # Enable sorting/filtering by default
            gridOptions = gb.build()
            # Add explicit initial sort (e.g., by Priority Tier)
            gridOptions['columnDefs'].append({'field': 'Priority_Tier', 'sort': 'desc', 'sortIndex': 0})

            # Display the AgGrid table
            grid_response = AgGrid(
                grid_display_df, gridOptions=gridOptions,
                data_return_mode=DataReturnMode.AS_INPUT, update_mode=GridUpdateMode.SELECTION_CHANGED,
                fit_columns_on_grid_load=False, allow_unsafe_jscode=False, enable_enterprise_modules=False,
                key='anomaly_grid_final_composite_merge_v3_buzei', # Updated key
                height=400, width='100%', reload_data=True
            )

            # --- Process AgGrid Selection --- (Logic remains the same)
            selected_data_df = None; new_selected_index = None
            if grid_response and 'selected_rows' in grid_response:
                 selected_data_df = pd.DataFrame(grid_response['selected_rows'])
            if selected_data_df is not None and not selected_data_df.empty:
                first_selected_row_series = selected_data_df.iloc[0]
                if 'original_index' in first_selected_row_series.index:
                    extracted_value = first_selected_row_series['original_index']
                    try: new_selected_index = df_anomalies.index.dtype.type(extracted_value)
                    except Exception as e:
                        print(f"Warning: Could not convert selected index '{extracted_value}' to type {df_anomalies.index.dtype}. Using raw value. Error: {e}")
                        new_selected_index = extracted_value
                else: print("Warning: 'original_index' column not found in selected row data.")
            elif grid_response and 'selected_rows' in grid_response and not grid_response['selected_rows']:
                 new_selected_index = None
            current_selection_in_state = st.session_state.get('selected_anomaly_index')
            selection_changed = (new_selected_index != current_selection_in_state)
            if selection_changed:
                st.session_state.selected_anomaly_index = new_selected_index
                print(f"DEBUG: Selection changed. New index: {new_selected_index}")
                st.rerun()

        except Exception as ag_err:
            st.error(f"Error displaying the anomaly list grid: {ag_err}")
            st.code(traceback.format_exc())
    else: # grid_display_df is empty
         if not df_anomalies.empty: st.caption("No anomalies match the current filter criteria.")


    # --- Anomaly Detail View ---
    st.markdown("---")
    st.subheader("Anomaly Detail")
    selected_index_from_state = st.session_state.get('selected_anomaly_index')

    if selected_index_from_state is not None:
        try:
            # Check if the selected index exists in the main anomaly DataFrame index
            if selected_index_from_state in df_anomalies.index:
                anomaly_detail_series = df_anomalies.loc[selected_index_from_state]
                anomaly_detail_dict = anomaly_detail_series.to_dict()
                anomaly_detail_dict['original_index'] = selected_index_from_state

                # Display Key Anomaly Info
                st.markdown(f"**Anomaly Reason:** {anomaly_detail_dict.get('Review_Focus', 'N/A')}")
                col1, col2 = st.columns(2)
                with col1: st.metric("Anomaly Priority", f"{anomaly_detail_dict.get('Priority_Tier', 'N/A')}")
                with col2: st.metric("Model Consensus", map_consensus_to_label(anomaly_detail_dict.get('Model_Anomaly_Count')))

                # Display Key Risk Flags
                active_hrf_flags = []
                for key, value in anomaly_detail_dict.items():
                    if key.startswith('HRF_') and (value == True or value == 1):
                         flag_name = ''.join([' ' + char if char.isupper() else char for char in key.replace('HRF_', '').replace('Is', '')]).strip()
                         active_hrf_flags.append(flag_name)
                if active_hrf_flags:
                    st.markdown("**Key Risk Flags Identified:**"); st.markdown(" | ".join(f"`{flag}`" for flag in sorted(active_hrf_flags)))

                st.markdown("---")
                st.markdown("**SAP Document Details:**")
                # *** ADD buzei to the display map if it was used ***
                core_sap_cols_display_map = {
                    'belnr': 'Document No.', 'gjahr': 'Fiscal Year',
                    # Conditionally include buzei
                    **({'buzei': 'Line Item'} if 'buzei' in final_merge_keys else {}),
                    'bukrs_correct_value': 'Company Code',
                    'budat_bkpf': 'Posting Date', 'blart': 'Document Type',
                    'tcode': 'Transaction Code', 'usnam_bkpf': 'User Name', 'racct': 'GL Account',
                    'hsl_correct_numeric': 'Amount (Local Curr. Numeric)',
                }

                # Prepare data for the detail display table
                detail_display_data = {}
                for k_orig, k_display in core_sap_cols_display_map.items():
                    detail_display_data[k_display] = [anomaly_detail_dict.get(k_orig, "N/A")]

                if detail_display_data:
                    detail_display_df = pd.DataFrame(detail_display_data)
                    # Apply formatting
                    if 'Amount (Local Curr. Numeric)' in detail_display_df.columns:
                        numeric_hsl_value = detail_display_df.loc[0, 'Amount (Local Curr. Numeric)']
                        bukrs_for_formatting = str(anomaly_detail_dict.get('bukrs_correct_value', ''))
                        currency_code_detail = bukrs_to_currency_info.get(bukrs_for_formatting, {}).get('code', '')
                        formatted_amount_str = format_large_currency(numeric_hsl_value, currency_code=currency_code_detail)
                        detail_display_df['Amount (Local Curr.) Formatted'] = formatted_amount_str
                        # Optionally hide numeric after formatting
                        # detail_display_df = detail_display_df.drop(columns=['Amount (Local Curr. Numeric)'])

                    if 'Posting Date' in detail_display_df.columns:
                         posting_date_val = pd.to_datetime(detail_display_df.loc[0, 'Posting Date'], errors='coerce')
                         detail_display_df.loc[0, 'Posting Date'] = posting_date_val.strftime('%d-%b-%Y') if pd.notna(posting_date_val) else 'N/A'

                    st.dataframe(detail_display_df) # Display formatted details
                else:
                    st.warning("Could not retrieve core SAP details for the selected anomaly.")

                # --- Display Additional Fields from Full Engineered File ---
                add_fields_found = False; full_details_row = None
                # Try indexed lookup first
                if df_full_engineered is not None and df_full_engineered.index.name == 'original_index' and selected_index_from_state in df_full_engineered.index:
                     try:
                         full_details_row = df_full_engineered.loc[[selected_index_from_state]]
                         add_fields_found = True; print(f"DEBUG: Found additional details using indexed df_full_engineered for index {selected_index_from_state}")
                     except Exception as lookup_err: print(f"DEBUG: Error looking up index {selected_index_from_state} in df_full_engineered: {lookup_err}")

                # *** Fallback uses the correct final_merge_keys ***
                if not add_fields_found and st.session_state.get('_temp_df_full') is not None:
                    print(f"DEBUG: Trying fallback lookup using merge keys {final_merge_keys} for index {selected_index_from_state}")
                    temp_full_for_merge = st.session_state['_temp_df_full']
                    try:
                        # Use final_merge_keys determined during loading
                        key_values = {k: [anomaly_detail_dict.get(k)] for k in final_merge_keys if k in anomaly_detail_dict}
                        if len(key_values) == len(final_merge_keys):
                            temp_key_df = pd.DataFrame(key_values)
                            # Ensure key types match the features DF for merging
                            temp_key_df['bukrs'] = temp_key_df['bukrs'].astype(str)
                            temp_key_df['belnr'] = temp_key_df['belnr'].astype(str)
                            temp_key_df['gjahr'] = pd.to_numeric(temp_key_df['gjahr'], errors='coerce').astype('Int64')
                            if 'buzei' in final_merge_keys: # Conditionally convert buzei
                                temp_key_df['buzei'] = pd.to_numeric(temp_key_df['buzei'], errors='coerce').astype('Int64')

                            full_details_row = pd.merge(temp_key_df, temp_full_for_merge, on=final_merge_keys, how='inner')
                            if not full_details_row.empty:
                                add_fields_found = True; print(f"DEBUG: Found additional details using fallback merge.")
                            else: print(f"DEBUG: Fallback merge did not find a match.")
                        else: print(f"DEBUG: Could not perform fallback merge, missing key values in anomaly_detail_dict.")
                    except Exception as merge_err: print(f"DEBUG: Error during fallback merge lookup: {merge_err}")

                # Display additional fields if found
                if add_fields_found and full_details_row is not None and not full_details_row.empty:
                    st.markdown("**Additional Fields (from Features Dataset):**")
                    core_sap_cols_shown = list(core_sap_cols_display_map.keys())
                    anomalies_context_cols = [c for c in df_anomalies.columns if c not in core_sap_cols_shown and not c.startswith(('hsl_correct','bukrs_correct','original_'))]
                    # Exclude keys used for merge (final_merge_keys)
                    exclude_cols = set(core_sap_cols_shown + anomalies_context_cols + final_merge_keys + ['hsl', 'bukrs', 'original_index', 'hsl_feature', 'bukrs_feature', 'Amount_Formatted', 'Model_Consensus_Label'])
                    exclude_cols.update([c for c in full_details_row.columns if c.startswith(('FE_', '_feature', '_correct'))])

                    show_additional_cols = [c for c in full_details_row.columns if c not in exclude_cols]
                    if show_additional_cols:
                        additional_df = full_details_row[show_additional_cols].iloc[[0]]
                        for col in additional_df.select_dtypes(include=['datetime64[ns]']).columns:
                             additional_df[col] = additional_df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
                        st.dataframe(additional_df.T.rename(columns={additional_df.index[0]: 'Value'}))
                        st.caption("*Fields sourced from the SAP Features file.*")
                    else:
                        st.caption("No additional relevant fields found in the Features file for this anomaly.")
                elif selected_index_from_state is not None:
                    st.caption("Could not find matching record in the SAP Features file to display additional details.")

            else: # Selected index not found
                 st.warning(f"Selected anomaly (Index: {selected_index_from_state}) not found in the current data. It might have been filtered out or removed.")
                 if st.session_state.selected_anomaly_index == selected_index_from_state:
                     st.session_state.selected_anomaly_index = None; st.rerun()
        except Exception as e:
             st.error(f"Error displaying anomaly details: {e}"); st.code(traceback.format_exc())
             st.session_state.selected_anomaly_index = None # Reset selection on error

    # --- Footer / No Selection Messages ---
    elif not filtered_df.empty: st.caption("Click a row in the 'Anomaly Investigation List' above to see its details.")
    elif not df_anomalies.empty and filtered_df.empty: st.caption("No anomalies match the current filters. Adjust filters to see data.")

# --- Initial State Message ---
elif not st.session_state.get('data_loaded', False) and not process_button:
     st.info("Upload the 'Prioritized Anomaly List' CSV and the 'SAP Features' CSV, then click 'Process Uploaded Files'.")