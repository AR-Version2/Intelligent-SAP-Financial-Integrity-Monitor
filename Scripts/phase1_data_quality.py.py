# -*- coding: utf-8 -*-
"""EnhancedFinDocAnomalyDetection_step1b_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ISvcHpFNCFlM4G84827v7sCCejRoizxA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt # Import added
import seaborn as sns           # Import added
import traceback # For detailed error printing
import os        # For path manipulation
import datetime  # For timestamp

# --- Configuration ---
# *** IMPORTANT: UPDATE THESE PATHS TO YOUR CLEANED FILES ***
output_directory = r"C:\Users\anith\Downloads\SAP DATASET from KAGGLE" # Directory where cleaned files are AND plots will be saved
bkpf_cleaned_filename = "bkpf_cleaned_after_step1_20250428_165811.csv" # <--- Using the confirmed filename
faglflexa_cleaned_filename = "faglflexa_cleaned_after_step1_20250428_165811.csv" # <--- Using the confirmed filename

bkpf_cleaned_file_path = os.path.join(output_directory, bkpf_cleaned_filename)
faglflexa_cleaned_file_path = os.path.join(output_directory, faglflexa_cleaned_filename)

# --- Pre-Cleaning Stats (Based on previous script output - Hardcoded for report) ---
# !! Update these if your previous run had different numbers !!
INITIAL_BKPF_ROWS = 150057
INITIAL_FAGLFLEXA_ROWS = 331934
BKPF_DUPLICATES_REMOVED = 13166
FAGLFLEXA_DUPLICATES_REMOVED = 40286

# --- Constants & Mappings (Needed for checks) ---
FAGLFLEXA_KEY_MAP = {
    'rbukrs': 'bukrs', 'docnr': 'belnr', 'ryear': 'gjahr', 'docln': 'buzei'
}
STD_KEY_NAMES = ['bukrs', 'belnr', 'gjahr']
FAGLFLEXA_NATIVE_ITEM_KEY_COLS = ['rbukrs', 'docnr', 'ryear', 'docln'] # Used to verify uniqueness

# --- Decision Logic Configuration (Copied from previous script for consistent criteria) ---
MAX_ACCEPTABLE_REDUNDANCY_MISMATCH_RATE = 0.01 # e.g., 1% mismatch allowed
MAX_ACCEPTABLE_PLACEHOLDER_RATE = 0.10 # e.g., Max 10% placeholders in key dimensions
PLACEHOLDER_VALUES = ['', '0000000000', '0', '/','__NaN__', None, '#', '000000'] # Add common placeholders found in your data (added '#', '000000')
KEY_FAGL_DIMENSIONS_FOR_GENUINENESS = ['rcntr', 'prctr', 'segment'] # Which FAGL dimensions must be "genuine"?

# --- Plotting Configuration ---
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (10, 6) # Default figure size
PLOT_TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
SAVE_PLOTS = True # Set to False if you don't want to save plots

# --- Helper Functions ---
def print_separator(title=""):
    """Prints a separator line with an optional title."""
    print("\n" + "="*30 + f" {title} " + "="*30)

def safe_to_datetime(series, **kwargs):
    return pd.to_datetime(series, errors='coerce', **kwargs)

def safe_to_numeric(series, **kwargs):
    return pd.to_numeric(series, errors='coerce', **kwargs)

def safe_to_string(series):
    if series is None: return None
    if pd.api.types.is_numeric_dtype(series):
        try: return series.astype(float).astype('Int64').astype(str)
        except (ValueError, TypeError): return series.astype(str)
    elif not pd.api.types.is_string_dtype(series): return series.astype(str)
    return series

def get_status_str(status_flag):
    if status_flag is True: return "OK"
    if status_flag is False: return "FAILED"
    return "Inconclusive/Skipped"

def format_value(value):
    """Formats numbers nicely for printing, handles NaN."""
    if pd.isna(value):
        return "N/A"
    if isinstance(value, float) and (abs(value) < 0.01 and value != 0):
        return f"{value:.2e}" # Scientific notation for very small floats
    if isinstance(value, (int, float)):
        return f"{value:,.0f}" # Comma separated integer
    return str(value)

# Global dictionary to store placeholder rates calculated by the function
placeholder_rate_info = {}

def check_population_and_genuineness(df, col_name, table_name, key_dimensions, placeholder_list, max_placeholder_rate):
    """Checks population rate and % of placeholders in key dimensions."""
    MIN_PLACEHOLDER_RATE_TO_REPORT = 0.0001
    if df is None or col_name not in df.columns:
         print(f"  {table_name}.{col_name}: Column not found or table invalid.")
         return 0.0, None
    total_rows = len(df)
    if total_rows == 0: print(f"  {table_name}.{col_name}: Table is empty."); return 0.0, None
    populated_count = df[col_name].fillna('').astype(str).str.strip().ne('').sum()
    population_rate = (populated_count / total_rows) if total_rows > 0 else 0
    print(f"  {table_name}.{col_name}: {population_rate:.2%} populated ({populated_count} / {total_rows})")
    genuineness_status = None; placeholder_rate = 0.0
    if col_name in key_dimensions:
        placeholders_str = {str(p) for p in placeholder_list if p is not None}
        if None in placeholder_list: placeholders_str.add('__NaN__')
        values_str = df[col_name].fillna('__NaN__').astype(str).str.strip()
        placeholder_count = values_str.isin(placeholders_str).sum()
        placeholder_rate = (placeholder_count / total_rows) if total_rows > 0 else 0
        if placeholder_rate >= MIN_PLACEHOLDER_RATE_TO_REPORT:
             display_placeholders = list(placeholders_str)[:5]
             print(f"    Placeholders (~{', '.join(display_placeholders)}...): Found in {placeholder_rate:.2%}")
        if placeholder_rate > max_placeholder_rate:
            genuineness_status = False
            print(f"    WARNING: Placeholder rate exceeds threshold ({max_placeholder_rate:.1%})")
        else: genuineness_status = True
    placeholder_rate_info[f"{table_name}.{col_name}"] = placeholder_rate
    return population_rate, genuineness_status

# --- **INSERTED FUNCTION DEFINITION HERE** ---
def show_value_counts(df, col_name, table_name, top_n=5):
     """Displays value counts for a column."""
     if df is None or col_name not in df.columns:
         print(f"\n{table_name}.{col_name}: Column not found or table invalid.")
         return
     print(f"\n{table_name}.{col_name} Distribution:")
     # Fillna with a placeholder string for counting NaNs explicitly
     print(df[col_name].fillna('__NaN__').value_counts(dropna=False).head(top_n))
# --- END OF INSERTED FUNCTION ---

def show_stats(df, cols, table_name):
    """Calculates stats and returns balance status (True/False/None)."""
    if df is None: print(f"\n{table_name}: Skipped numeric stats."); return None
    print(f"\n{table_name} Amount Statistics:")
    numeric_cols_present = [col for col in cols if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
    balance_status = None
    if numeric_cols_present:
        try: print(df[numeric_cols_present].describe())
        except Exception as e: print(f"  Error during describe(): {e}")
        local_curr_col = None
        if 'hsl' in numeric_cols_present and table_name.startswith('FAGLFLEXA'): local_curr_col = 'hsl'
        if local_curr_col:
            if df[local_curr_col].notna().any():
                 try:
                     total_balance = df[local_curr_col].sum(skipna=True)
                     is_balanced = np.isclose(total_balance, 0, atol=0.01)
                     balance_status = is_balanced
                     print(f"  Sum of {local_curr_col} (Local Curr): {total_balance:,.2f} {'(OK: Balanced)' if is_balanced else '(WARNING: Imbalanced!)'}")
                 except Exception as e: print(f"  Error during balance calc: {e}"); balance_status = None
            else: print(f"  Sum of {local_curr_col}: Column has only NaN values."); balance_status = None
        else: print("  Local currency col (hsl) not found/numeric."); balance_status = None
    else: print("  No numeric amount columns found/processed."); balance_status = None
    return balance_status

# --- Visualization Helper Functions ---
def save_plot(plt_obj, filename_base, output_dir):
    """Saves the current matplotlib plot with timestamp."""
    if SAVE_PLOTS:
        try:
            filename = f"{filename_base}_{PLOT_TIMESTAMP}.png"
            path = os.path.join(output_dir, filename)
            if not os.path.exists(output_dir): os.makedirs(output_dir)
            plt_obj.savefig(path, bbox_inches='tight', dpi=150); print(f"   * Plot saved: {filename}")
        except Exception as e: print(f"   * WARNING: Could not save plot {filename_base}. Error: {e}")
    plt_obj.close()

def plot_row_counts(initial_bkpf, cleaned_bkpf, initial_fagl, cleaned_fagl, output_dir):
    """Plots original vs cleaned row counts."""
    labels = ['BKPF', 'FAGLFLEXA']; initial_counts = [initial_bkpf, initial_fagl]; cleaned_counts = [cleaned_bkpf, cleaned_fagl]
    x = np.arange(len(labels)); width = 0.35; fig, ax = plt.subplots()
    rects1 = ax.bar(x - width/2, initial_counts, width, label='Original', color='skyblue'); rects2 = ax.bar(x + width/2, cleaned_counts, width, label='Cleaned', color='lightcoral')
    ax.set_ylabel('Row Count'); ax.set_title('Row Counts Before and After Cleaning'); ax.set_xticks(x); ax.set_xticklabels(labels); ax.legend()
    ax.bar_label(rects1, padding=3, fmt='{:,.0f}'); ax.bar_label(rects2, padding=3, fmt='{:,.0f}'); fig.tight_layout(); save_plot(plt, "row_counts_comparison", output_dir)

def plot_duplicates_removed(bkpf_removed, fagl_removed, output_dir):
    """Plots the number of duplicate rows removed."""
    labels = ['BKPF', 'FAGLFLEXA']; removed_counts = [bkpf_removed, fagl_removed]
    fig, ax = plt.subplots(); rects = ax.bar(labels, removed_counts, color=['skyblue', 'lightcoral'])
    ax.set_ylabel('Duplicate Rows Removed'); ax.set_title('Duplicate Rows Removed During Cleaning')
    ax.bar_label(rects, padding=3, fmt='{:,.0f}'); fig.tight_layout(); save_plot(plt, "duplicates_removed", output_dir)

def plot_dimension_population(pop_data, table_name, output_dir):
    """Plots population rate for dimensions."""
    if not pop_data: return
    labels = list(pop_data.keys()); rates = [r * 100 for r in pop_data.values()]
    fig, ax = plt.subplots(); rects = ax.bar(labels, rates, color='mediumseagreen')
    ax.set_ylabel('Population Rate (%)'); ax.set_title(f'Dimension Population Rate ({table_name} Cleaned)')
    ax.set_ylim(0, 105); ax.bar_label(rects, padding=3, fmt='%.2f%%'); plt.xticks(rotation=30, ha='right'); fig.tight_layout()
    save_plot(plt, f"{table_name.lower()}_dimension_population", output_dir)

def plot_placeholder_rates(placeholder_data, table_name, output_dir):
    """Plots placeholder rate for key dimensions."""
    if not placeholder_data: return
    labels = list(placeholder_data.keys()); rates = [r * 100 for r in placeholder_data.values()]
    fig, ax = plt.subplots(); rects = ax.bar(labels, rates, color='lightcoral')
    ax.set_ylabel('Placeholder Rate (%)'); ax.set_title(f'Placeholder Rate in Key Dimensions ({table_name} Cleaned)')
    max_rate = max(rates) if rates else 0; ax.set_ylim(0, max(max_rate * 1.2, 1)); ax.bar_label(rects, padding=3, fmt='%.2f%%')
    plt.xticks(rotation=30, ha='right'); fig.tight_layout(); save_plot(plt, f"{table_name.lower()}_placeholder_rates", output_dir)

def plot_amount_distribution(series, col_name, table_name, output_dir):
    """Plots histogram or box plot for an amount column."""
    if series is None or series.dropna().empty: return
    plt.figure(); non_zero_data = series.dropna()[series.dropna() != 0]; use_log = False
    if not non_zero_data.empty:
        min_val = non_zero_data.min(); max_val = non_zero_data.max()
        if min_val > 0 and max_val / min_val > 1000: use_log = True
        elif (max_val - min_val) > 0 and (abs(max_val) > 1000 * abs(min_val) if min_val != 0 else abs(max_val) > 100): use_log = 'symlog_y'
    if use_log == True: sns.histplot(series.dropna(), bins=100, kde=False, log_scale=True); plt.title(f'Distribution of {col_name} ({table_name} Cleaned) - Log Scale')
    elif use_log == 'symlog_y': sns.histplot(series.dropna(), bins=100, kde=False); plt.yscale('symlog'); plt.title(f'Distribution of {col_name} ({table_name} Cleaned) - Symlog Y-axis')
    else: sns.histplot(series.dropna(), bins=50, kde=True); plt.title(f'Distribution of {col_name} ({table_name} Cleaned)')
    plt.xlabel(col_name); plt.ylabel('Frequency'); save_plot(plt, f"{table_name.lower()}_{col_name}_distribution", output_dir)

def plot_redundancy_summary(mismatch_data, output_dir):
    """Creates a simple bar chart of redundancy mismatches."""
    if not mismatch_data: return
    labels = list(mismatch_data.keys()); counts = list(mismatch_data.values())
    fig, ax = plt.subplots(figsize=(12, 6)); rects = ax.bar(labels, counts, color='orange')
    ax.set_ylabel('Mismatch Count'); ax.set_title('Redundancy Check Mismatches (FAGLFLEXA vs BKPF Cleaned)')
    ax.bar_label(rects, padding=3, fmt='{:,.0f}'); plt.xticks(rotation=45, ha='right'); fig.tight_layout()
    save_plot(plt, "redundancy_mismatches", output_dir)


# --- 1. Load CLEANED Data ---
print_separator("Loading Cleaned Data")
# ... (Loading logic) ...
df_bkpf_clean = None; df_faglflexa_clean = None; bkpf_clean_ok = False; faglflexa_clean_ok = False
try:
    print(f"Attempting to load cleaned BKPF from: {bkpf_cleaned_file_path}"); df_bkpf_clean = pd.read_csv(bkpf_cleaned_file_path, low_memory=False); bkpf_clean_ok = True; print(f"Loaded Cleaned BKPF: {df_bkpf_clean.shape}")
except FileNotFoundError: print(f"CRITICAL ERROR: Cleaned BKPF file not found at {bkpf_cleaned_file_path}"); bkpf_clean_ok = False
except Exception as e: print(f"CRITICAL ERROR: Error loading cleaned BKPF data: {e}"); traceback.print_exc(); bkpf_clean_ok = False
try:
    print(f"Attempting to load cleaned FAGLFLEXA from: {faglflexa_cleaned_file_path}"); df_faglflexa_clean = pd.read_csv(faglflexa_cleaned_file_path, low_memory=False); faglflexa_clean_ok = True; print(f"Loaded Cleaned FAGLFLEXA: {df_faglflexa_clean.shape}")
except FileNotFoundError: print(f"CRITICAL ERROR: Cleaned FAGLFLEXA file not found at {faglflexa_cleaned_file_path}"); faglflexa_clean_ok = False
except Exception as e: print(f"CRITICAL ERROR: Error loading cleaned FAGLFLEXA data: {e}"); traceback.print_exc(); faglflexa_clean_ok = False
if not bkpf_clean_ok or not faglflexa_clean_ok: print("\nCRITICAL ERROR: Failed to load one or both cleaned data files."); exit()
CLEANED_BKPF_ROWS = len(df_bkpf_clean) if bkpf_clean_ok else 0; CLEANED_FAGLFLEXA_ROWS = len(df_faglflexa_clean) if faglflexa_clean_ok else 0

# --- 2. Data Type Conversion ---
print_separator("Data Type Conversion on Cleaned Data")
# ... (Type conversion logic) ...
print("Converting BKPF types...");
if bkpf_clean_ok:
    key_cols = ['bukrs', 'belnr', 'gjahr']; date_cols = ['bldat', 'budat', 'cpudt']; num_cols = ['kursf']
    str_cols = ['blart', 'monat', 'usnam', 'tcode', 'xblnr', 'bktxt', 'waers', 'hwaer', 'awtyp', 'awkey', 'cputm']
    for col in key_cols+date_cols+num_cols+str_cols:
        if col in df_bkpf_clean.columns:
            if col in key_cols+str_cols: df_bkpf_clean[col] = safe_to_string(df_bkpf_clean[col])
            elif col in date_cols: df_bkpf_clean[col] = safe_to_datetime(df_bkpf_clean[col], format='mixed')
            elif col in num_cols: df_bkpf_clean[col] = safe_to_numeric(df_bkpf_clean[col])
print("Converting FAGLFLEXA types...");
if faglflexa_clean_ok:
    key_cols = list(FAGLFLEXA_KEY_MAP.keys()); redundant_keys = ['belnr', 'buzei', 'gjahr']; date_cols = ['budat']; num_cols = ['hsl', 'tsl']
    str_cols = ['rldnr', 'racct', 'drcrk', 'rtcur', 'rwcur', 'rcntr', 'prctr', 'segment', 'rfarea', 'rbusa', 'kokrs', 'poper', 'usnam', 'awtyp']
    for col in key_cols + redundant_keys + date_cols + num_cols + str_cols:
         if col in df_faglflexa_clean.columns:
            if col in key_cols+redundant_keys+str_cols: df_faglflexa_clean[col] = safe_to_string(df_faglflexa_clean[col])
            elif col in date_cols: df_faglflexa_clean[col] = safe_to_datetime(df_faglflexa_clean[col], format='mixed')
            elif col in num_cols: df_faglflexa_clean[col] = safe_to_numeric(df_faglflexa_clean[col])

# --- 3. Data Quality Analysis on Cleaned Data ---
print_separator("Data Quality Analysis of Cleaned Data")
# ... (Initialize Status Variables) ...
bkpf_duplicates_clean = 0; fagl_duplicates_clean = 0; fagl_orphans_clean = 0
fagl_balanced_clean = None; redundancy_ok_clean = None; fagl_dimensions_genuine_clean = None
fagl_pop_rates = {}; fagl_placeholder_rates = {}; redundancy_mismatches_clean = {}

# --- 3.1 Uniqueness Checks ---
print("\n--- Uniqueness Checks (Verification on Cleaned Data) ---")
# ... (Uniqueness checks) ...
if bkpf_clean_ok:
    bkpf_key_actual = [col for col in STD_KEY_NAMES if col in df_bkpf_clean.columns]
    if len(bkpf_key_actual) == len(STD_KEY_NAMES): bkpf_duplicates_clean = df_bkpf_clean.dropna(subset=bkpf_key_actual).duplicated(subset=bkpf_key_actual).sum(); print(f"BKPF Clean Duplicates: {bkpf_duplicates_clean} ({'OK' if bkpf_duplicates_clean == 0 else 'ERROR!'})")
    else: print("BKPF Clean: Cannot check uniqueness.")
if faglflexa_clean_ok:
    fagl_key_actual_original = [col for col in FAGLFLEXA_NATIVE_ITEM_KEY_COLS if col in df_faglflexa_clean.columns]
    if len(fagl_key_actual_original) == len(FAGLFLEXA_NATIVE_ITEM_KEY_COLS): fagl_duplicates_clean = df_faglflexa_clean.dropna(subset=fagl_key_actual_original).duplicated(subset=fagl_key_actual_original).sum(); print(f"FAGLFLEXA Clean Duplicates: {fagl_duplicates_clean} ({'OK' if fagl_duplicates_clean == 0 else 'ERROR!'})")
    else: print("FAGLFLEXA Clean: Cannot check uniqueness.")

# --- 3.2 Consistency Checks ---
print("\n--- Consistency Checks (FAGLFLEXA Clean vs BKPF Clean) ---")
# ... (Consistency checks) ...
if bkpf_clean_ok and faglflexa_clean_ok and len([k for k in STD_KEY_NAMES if k in df_bkpf_clean.columns]) == len(STD_KEY_NAMES):
    bkpf_keys_unique_clean = df_bkpf_clean[STD_KEY_NAMES].dropna().drop_duplicates()
    fagl_key_actual_original = list(FAGLFLEXA_KEY_MAP.keys()); fagl_header_keys_original = [k for k in fagl_key_actual_original if k != 'docln']
    if all(k in df_faglflexa_clean.columns for k in fagl_header_keys_original):
        fagl_keys_renamed = df_faglflexa_clean[fagl_header_keys_original].dropna().drop_duplicates(); rename_map_header = {k:v for k,v in FAGLFLEXA_KEY_MAP.items() if k in fagl_header_keys_original}; fagl_keys_renamed = fagl_keys_renamed.rename(columns=rename_map_header)
        if len([k for k in STD_KEY_NAMES if k in fagl_keys_renamed.columns]) == len(STD_KEY_NAMES):
            fagl_merged_clean = pd.merge(fagl_keys_renamed, bkpf_keys_unique_clean, on=STD_KEY_NAMES, how='left', indicator=True); fagl_orphans_clean = (fagl_merged_clean['_merge'] == 'left_only').sum()
            print(f"FAGLFLEXA Clean Orphans: {fagl_orphans_clean} ({'OK' if fagl_orphans_clean == 0 else 'WARNING'})")
        else: print("FAGLFLEXA Clean: Skipped consistency (rename failed).")
    else: print("FAGLFLEXA Clean: Skipped consistency (missing keys).")
else: print("Skipped consistency checks.")


# --- 3.3 Population Rate & Dimension Genuineness ---
print("\n--- Population Rate & Dimension Genuineness Checks (FAGLFLEXA Clean) ---")
fagl_dim_genuine_flags_clean = []
if faglflexa_clean_ok:
    print("\nFAGLFLEXA Clean Population & Genuineness:")
    # Use the check_population_and_genuineness function
    for col in ['rcntr', 'prctr', 'segment', 'rfarea', 'rbusa']:
         pop_rate, genuine_status = check_population_and_genuineness(
             df_faglflexa_clean, col, "FAGLFLEXA Clean", KEY_FAGL_DIMENSIONS_FOR_GENUINENESS, PLACEHOLDER_VALUES, MAX_ACCEPTABLE_PLACEHOLDER_RATE
         )
         fagl_pop_rates[col] = pop_rate
         if col in KEY_FAGL_DIMENSIONS_FOR_GENUINENESS: fagl_dim_genuine_flags_clean.append(genuine_status)
    if not KEY_FAGL_DIMENSIONS_FOR_GENUINENESS: fagl_dimensions_genuine_clean = True
    elif not fagl_dim_genuine_flags_clean: fagl_dimensions_genuine_clean = None
    elif all(status == True for status in fagl_dim_genuine_flags_clean): fagl_dimensions_genuine_clean = True
    else: fagl_dimensions_genuine_clean = False
else: print("FAGLFLEXA Clean: Skipped population checks.")

# --- 3.4 Value Distribution Checks ---
print("\n--- Value Distribution Checks (Cleaned Data - Top 5) ---")
# ** This is where the function call happens **
if bkpf_clean_ok: show_value_counts(df_bkpf_clean, 'blart', 'BKPF Clean'); show_value_counts(df_bkpf_clean, 'usnam', 'BKPF Clean'); show_value_counts(df_bkpf_clean, 'tcode', 'BKPF Clean')
if faglflexa_clean_ok: show_value_counts(df_faglflexa_clean, 'rldnr', 'FAGLFLEXA Clean'); show_value_counts(df_faglflexa_clean, 'racct', 'FAGLFLEXA Clean'); show_value_counts(df_faglflexa_clean, 'drcrk', 'FAGLFLEXA Clean')

# --- 3.5 Numeric Statistics ---
print("\n--- Numeric Statistics (FAGLFLEXA Clean Amounts) ---")
# ... (Call to show_stats) ...
if faglflexa_clean_ok: fagl_balanced_clean = show_stats(df_faglflexa_clean, ['hsl', 'tsl'], 'FAGLFLEXA Clean')
else: print("FAGLFLEXA Clean: Skipped numeric stats.")

# --- 3.6 Redundancy Checks ---
print("\n--- Redundancy Checks (FAGLFLEXA Clean vs BKPF Clean) ---")
# ... (Redundancy checks) ...
total_compared_records_clean = 0; total_mismatches_clean = 0; redundancy_ok_clean = None; redundancy_mismatches_clean = {}
if faglflexa_clean_ok and bkpf_clean_ok:
    # ... (subset prep) ...
    bkpf_compare_cols_clean = STD_KEY_NAMES + ['budat', 'usnam', 'awtyp']; bkpf_compare_cols_present_clean = [col for col in bkpf_compare_cols_clean if col in df_bkpf_clean.columns]; bkpf_subset_clean = None
    if len([k for k in STD_KEY_NAMES if k in bkpf_compare_cols_present_clean]) == len(STD_KEY_NAMES): bkpf_subset_clean = df_bkpf_clean[bkpf_compare_cols_present_clean].dropna(subset=STD_KEY_NAMES).drop_duplicates(subset=STD_KEY_NAMES)
    fagl_original_keys_clean = ['rbukrs', 'docnr', 'ryear']; fagl_redundant_fields_clean = ['gjahr', 'budat', 'belnr', 'usnam', 'awtyp']; fagl_compare_cols_clean = fagl_original_keys_clean + fagl_redundant_fields_clean; fagl_compare_cols_present_clean = [col for col in fagl_compare_cols_clean if col in df_faglflexa_clean.columns]; fagl_subset_for_merge_clean = None
    if len([k for k in fagl_original_keys_clean if k in fagl_compare_cols_present_clean]) == len(fagl_original_keys_clean):
        fagl_subset_orig_clean = df_faglflexa_clean[fagl_compare_cols_present_clean].dropna(subset=fagl_original_keys_clean).drop_duplicates(subset=fagl_original_keys_clean).copy()
        if 'belnr' in fagl_subset_orig_clean.columns: fagl_subset_orig_clean = fagl_subset_orig_clean.rename(columns={'belnr': 'belnr_original_fagl'})
        rename_map_clean = {k: v for k, v in FAGLFLEXA_KEY_MAP.items() if k in fagl_subset_orig_clean.columns and k in fagl_original_keys_clean}
        fagl_subset_for_merge_clean = fagl_subset_orig_clean.rename(columns=rename_map_clean)
        if len([k for k in STD_KEY_NAMES if k in fagl_subset_for_merge_clean.columns]) != len(STD_KEY_NAMES): fagl_subset_for_merge_clean = None
    if bkpf_subset_clean is not None and fagl_subset_for_merge_clean is not None:
        # ... (merge logic) ...
        try:
            if not fagl_subset_for_merge_clean.columns.is_unique: fagl_subset_for_merge_clean = fagl_subset_for_merge_clean.loc[:, ~fagl_subset_for_merge_clean.columns.duplicated()]
            compare_df_clean = pd.merge(fagl_subset_for_merge_clean, bkpf_subset_clean, on=STD_KEY_NAMES, how='inner', suffixes=('_fagl', '_bkpf'))
            total_compared_records_clean = len(compare_df_clean)
            if not compare_df_clean.empty:
                num_compared_fields_clean = 0; # Comparisons ...
                field_name = 'Fiscal Year (FAGL.gjahr vs BKPF.gjahr)'; current_field_mismatches=0
                if 'gjahr_fagl' in compare_df_clean.columns and 'gjahr_bkpf' in compare_df_clean.columns: gjahr_fagl_str = safe_to_string(compare_df_clean['gjahr_fagl'].fillna('')); gjahr_bkpf_str = safe_to_string(compare_df_clean['gjahr_bkpf'].fillna('')); current_field_mismatches = (gjahr_fagl_str != gjahr_bkpf_str).sum(); redundancy_mismatches_clean[field_name] = current_field_mismatches; num_compared_fields_clean+=1; print(f"      {field_name} Mismatches: {current_field_mismatches} ({'OK' if current_field_mismatches==0 else 'WARNING'})")
                field_name = 'Posting Date (FAGL.budat vs BKPF.budat)'; current_field_mismatches=0
                if 'budat_fagl' in compare_df_clean.columns and 'budat_bkpf' in compare_df_clean.columns: date1 = pd.to_datetime(compare_df_clean['budat_fagl'], errors='coerce'); date2 = pd.to_datetime(compare_df_clean['budat_bkpf'], errors='coerce'); valid_comparison = date1.notna() & date2.notna(); current_field_mismatches = (date1[valid_comparison] != date2[valid_comparison]).sum(); redundancy_mismatches_clean[field_name] = current_field_mismatches; num_compared_fields_clean+=1; print(f"      {field_name} Mismatches: {current_field_mismatches} ({'OK' if current_field_mismatches==0 else 'WARNING'})")
                field_name = 'Doc Number (FAGL.belnr_original vs BKPF.belnr Key)'; current_field_mismatches=0
                original_fagl_belnr_col = 'belnr_original_fagl'; bkpf_key_belnr_col = 'belnr'
                if original_fagl_belnr_col in compare_df_clean.columns and bkpf_key_belnr_col in compare_df_clean.columns: belnr_fagl_str = safe_to_string(compare_df_clean[original_fagl_belnr_col].fillna('')); belnr_bkpf_key_str = safe_to_string(compare_df_clean[bkpf_key_belnr_col].fillna('')); current_field_mismatches = (belnr_fagl_str != belnr_bkpf_key_str).sum(); redundancy_mismatches_clean[field_name] = current_field_mismatches; num_compared_fields_clean+=1; print(f"      {field_name} Mismatches: {current_field_mismatches} ({'OK' if current_field_mismatches==0 else 'WARNING'})")
                field_name = 'User Name (FAGL.usnam vs BKPF.usnam)'; current_field_mismatches=0
                if 'usnam_fagl' in compare_df_clean.columns and 'usnam_bkpf' in compare_df_clean.columns: usnam_fagl_str = safe_to_string(compare_df_clean['usnam_fagl'].fillna('')); usnam_bkpf_str = safe_to_string(compare_df_clean['usnam_bkpf'].fillna('')); current_field_mismatches = (usnam_fagl_str != usnam_bkpf_str).sum(); redundancy_mismatches_clean[field_name] = current_field_mismatches; num_compared_fields_clean+=1; print(f"      {field_name} Mismatches: {current_field_mismatches} ({'OK' if current_field_mismatches==0 else 'WARNING'})")
                field_name = 'Ref Procedure (FAGL.awtyp vs BKPF.awtyp)'; current_field_mismatches=0
                if 'awtyp_fagl' in compare_df_clean.columns and 'awtyp_bkpf' in compare_df_clean.columns: awtyp_fagl_str = safe_to_string(compare_df_clean['awtyp_fagl'].fillna('')); awtyp_bkpf_str = safe_to_string(compare_df_clean['awtyp_bkpf'].fillna('')); current_field_mismatches = (awtyp_fagl_str != awtyp_bkpf_str).sum(); redundancy_mismatches_clean[field_name] = current_field_mismatches; num_compared_fields_clean+=1; print(f"      {field_name} Mismatches: {current_field_mismatches} ({'OK' if current_field_mismatches==0 else 'WARNING'})")

                total_mismatches_clean = sum(redundancy_mismatches_clean.values())
                if total_compared_records_clean > 0 and num_compared_fields_clean > 0:
                     mismatch_rate_clean = (total_mismatches_clean / total_compared_records_clean / num_compared_fields_clean)
                     redundancy_ok_clean = mismatch_rate_clean <= MAX_ACCEPTABLE_REDUNDANCY_MISMATCH_RATE
                     print(f"    Overall redundancy mismatch rate ({mismatch_rate_clean:.2%}) is {'within' if redundancy_ok_clean else 'ABOVE'} threshold. ({'OK' if redundancy_ok_clean else 'WARNING'})")
                else: redundancy_ok_clean = None
        except Exception as e: print(f"  ERROR during redundancy merge/compare: {e}"); traceback.print_exc(); redundancy_ok_clean = None
    else: print("  Skipped redundancy merge."); redundancy_ok_clean = None
else: print("Skipped redundancy checks."); redundancy_ok_clean = None


# --- 4. Generate Report Summaries and Visualizations ---
print_separator("Generating Data Quality Report Summaries & Visualizations")
# ... (Calls to plot functions and print summary) ...
print("\n--- Quantitative Summary ---"); print("Initial Data:"); print(f"  BKPF Rows: {format_value(INITIAL_BKPF_ROWS)}"); print(f"  FAGLFLEXA Rows: {format_value(INITIAL_FAGLFLEXA_ROWS)}")
print("\nCleaning Step:"); print(f"  BKPF Duplicates Removed: {format_value(BKPF_DUPLICATES_REMOVED)} ({BKPF_DUPLICATES_REMOVED/INITIAL_BKPF_ROWS:.2%})"); print(f"  FAGLFLEXA Duplicates Removed: {format_value(FAGLFLEXA_DUPLICATES_REMOVED)} ({FAGLFLEXA_DUPLICATES_REMOVED/INITIAL_FAGLFLEXA_ROWS:.2%})"); print(f"  BSEG Table: Discarded.")
print("\nCleaned Data:"); print(f"  BKPF Rows: {format_value(CLEANED_BKPF_ROWS)}"); print(f"  FAGLFLEXA Rows: {format_value(CLEANED_FAGLFLEXA_ROWS)}")
print("\nCleaned Data Quality Checks:"); print(f"  BKPF Uniqueness: {get_status_str(bkpf_duplicates_clean == 0)}"); print(f"  FAGLFLEXA Uniqueness: {get_status_str(fagl_duplicates_clean == 0)}"); print(f"  FAGLFLEXA Consistency (vs BKPF): {get_status_str(fagl_orphans_clean == 0)} ({format_value(fagl_orphans_clean)} orphans)"); print(f"  FAGLFLEXA Balance (Sum HSL): {get_status_str(fagl_balanced_clean)}"); print(f"  FAGLFLEXA Dimension Genuineness: {get_status_str(fagl_dimensions_genuine_clean)}"); print(f"  FAGLFLEXA Redundancy (vs BKPF): {get_status_str(redundancy_ok_clean)}")
if redundancy_mismatches_clean:
    for field, count in redundancy_mismatches_clean.items(): print(f"    - {field}: {format_value(count)} mismatches")
print("\n--- Generating Visualizations ---");
try:
    plot_row_counts(INITIAL_BKPF_ROWS, CLEANED_BKPF_ROWS, INITIAL_FAGLFLEXA_ROWS, CLEANED_FAGLFLEXA_ROWS, output_directory)
    plot_duplicates_removed(BKPF_DUPLICATES_REMOVED, FAGLFLEXA_DUPLICATES_REMOVED, output_directory)
    if faglflexa_clean_ok:
        plot_dimension_population(fagl_pop_rates, "FAGLFLEXA", output_directory)
        plot_placeholder_rates({k.split('.')[-1]:v for k,v in placeholder_rate_info.items() if k.startswith('FAGLFLEXA Clean') and k.split('.')[-1] in KEY_FAGL_DIMENSIONS_FOR_GENUINENESS}, "FAGLFLEXA Key", output_directory)
        if 'hsl' in df_faglflexa_clean.columns: plot_amount_distribution(df_faglflexa_clean['hsl'], 'hsl', 'FAGLFLEXA', output_directory)
    if redundancy_mismatches_clean: plot_redundancy_summary(redundancy_mismatches_clean, output_directory)
    print("   * Visualizations generated successfully (if applicable).")
except Exception as e: print(f"   * WARNING: Error generating visualizations: {e}"); traceback.print_exc()
print("\n--- Sample Data (Cleaned) ---"); pd.set_option('display.max_columns', 50); pd.set_option('display.width', 1000)
print("\nSample BKPF Clean Data (10 rows):");
if bkpf_clean_ok: print(df_bkpf_clean.sample(n=min(10, len(df_bkpf_clean)), random_state=42))
else: print("BKPF Clean data not available.")
print("\nSample FAGLFLEXA Clean Data (10 rows):");
if faglflexa_clean_ok: print(df_faglflexa_clean.sample(n=min(10, len(df_faglflexa_clean)), random_state=42))
else: print("FAGLFLEXA Clean data not available.")


# --- 5. Final Assessment ---
print_separator("Final Assessment of Cleaned BKPF + FAGLFLEXA Data")
# ... (Final assessment logic) ...
final_assessment_notes = []; cleaned_data_suitable = False
bkpf_unique_ok = (bkpf_duplicates_clean == 0) if bkpf_clean_ok else None; fagl_unique_ok = (fagl_duplicates_clean == 0) if faglflexa_clean_ok else None
fagl_consistency_ok = (fagl_orphans_clean == 0) if (bkpf_clean_ok and faglflexa_clean_ok and 'fagl_orphans_clean' in locals()) else None
fagl_balance_final_ok = fagl_balanced_clean == True if faglflexa_clean_ok else None; fagl_dimension_final_ok = fagl_dimensions_genuine_clean == True if faglflexa_clean_ok else None; fagl_redundancy_final_ok = redundancy_ok_clean == True if (bkpf_clean_ok and faglflexa_clean_ok) else None
final_assessment_notes.append(f"BKPF Uniqueness: {get_status_str(bkpf_unique_ok)}"); final_assessment_notes.append(f"FAGLFLEXA Uniqueness: {get_status_str(fagl_unique_ok)}")
final_assessment_notes.append(f"FAGLFLEXA Consistency vs BKPF: {get_status_str(fagl_consistency_ok)} {'('+str(fagl_orphans_clean)+' orphans)' if fagl_consistency_ok==False else ''}")
final_assessment_notes.append(f"FAGLFLEXA Financial Balance: {get_status_str(fagl_balance_final_ok)}"); final_assessment_notes.append(f"FAGLFLEXA Dimension Genuineness: {get_status_str(fagl_dimension_final_ok)}"); final_assessment_notes.append(f"FAGLFLEXA Redundancy vs BKPF: {get_status_str(fagl_redundancy_final_ok)}")
mandatory_checks_passed = (bkpf_unique_ok == True and fagl_unique_ok == True and fagl_consistency_ok == True and fagl_balance_final_ok == True)
if mandatory_checks_passed:
    cleaned_data_suitable = True; final_assessment_notes.append("\nConclusion: The cleaned BKPF and FAGLFLEXA dataset appears suitable for feature engineering."); final_assessment_notes.append("  -> Core DQ checks (uniqueness, consistency, balance) passed.")
    if fagl_dimension_final_ok is not True: final_assessment_notes.append(f"  -> WARNING: Dimension genuineness check: {get_status_str(fagl_dimension_final_ok)} (check details).")
    if fagl_redundancy_final_ok is not True: final_assessment_notes.append(f"  -> WARNING: Redundancy check: {get_status_str(fagl_redundancy_final_ok)} (check details).")
else:
    cleaned_data_suitable = False; final_assessment_notes.append("\nConclusion: The cleaned BKPF and FAGLFLEXA dataset STILL has issues preventing direct use.")
    if bkpf_unique_ok is False or fagl_unique_ok is False: final_assessment_notes.append("  -> CRITICAL: Duplicates REMAIN - investigate.")
    elif fagl_consistency_ok is False: final_assessment_notes.append("  -> CRITICAL: Orphaned FAGLFLEXA records remain - investigate.")
    elif fagl_balance_final_ok is False: final_assessment_notes.append("  -> CRITICAL: FAGLFLEXA financial balance is incorrect - investigate.")
    else: final_assessment_notes.append("  -> Review specific check failures above (Dimensions, Redundancy, or inconclusive checks).")
    final_assessment_notes.append("  Recommendation: Further investigation/cleaning needed OR request new data.")
for note in final_assessment_notes: print(note)
if cleaned_data_suitable: print("\nNEXT STEP: Proceed to Feature Engineering using the loaded 'df_bkpf_clean' and 'df_faglflexa_clean' DataFrames.")
else: print("\nNEXT STEP: Address the remaining data quality issues identified above before feature engineering.")

print("\nCleaned Data Quality Analysis Report Generation Complete.")