# -*- coding: utf-8 -*-
"""phase3_step4_baseline_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hh1FHxKL6sVUUW5xEf7CTQSKrYE46E3q
"""

import pandas as pd
import numpy as np
import traceback
import os
import datetime

# --- Configuration ---
# Input Directory (Where previous outputs are located)
input_directory = r"C:\Users\anith\Downloads"

# Input Files (Need original features AND baseline results)
engineered_features_filename = "sap_engineered_features__20250428_205234.csv" # Original engineered features
# *** IMPORTANT: Use the timestamp from the PREVIOUS script's output ***
PREVIOUS_TIMESTAMP = "20250428_230602" # Timestamp from phase3_step3_prep_and_baseline.py output

results_filename = f"phase3_step4a_baseline_results_{PREVIOUS_TIMESTAMP}.csv"

input_engineered_path = os.path.join(input_directory, engineered_features_filename)
input_results_path = os.path.join(input_directory, results_filename)

# Output Directory (Optional - for saving profiled anomalies)
output_directory = r"C:\Users\anith\Downloads"
SAVE_ANOMALY_SUBSETS = True # Set to True to save CSVs of flagged anomalies

# Output File Naming
SCRIPT_NAME = "baseline_eval" # Base name for output files from this script
PHASE_NUMBER = 3
STEP_EVAL = "4b"
TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
if_anomaly_filename = f"phase{PHASE_NUMBER}_step{STEP_EVAL}_if_anomalies_profiled_{TIMESTAMP}.csv"
lof_anomaly_filename = f"phase{PHASE_NUMBER}_step{STEP_EVAL}_lof_anomalies_profiled_{TIMESTAMP}.csv"


# --- Helper Functions ---
def print_separator(title=""):
    print("\n" + "="*30 + f" {title} " + "="*30)

def format_value(value):
    """Formats numbers nicely for printing, handles NaN."""
    if pd.isna(value): return "N/A"
    if isinstance(value, float) and (abs(value) < 0.01 and value != 0): return f"{value:.2e}"
    if isinstance(value, (int, float)): return f"{value:,.0f}"
    return str(value)

def compare_feature_stats(df, label_col, feature_list, compare_type='numerical'):
    """ Compares stats (mean/median or value_counts) for anomalies vs normal """
    anomalies = df[df[label_col] == -1]
    normal = df[df[label_col] == 1]
    print(f"\n--- Comparing Features for {label_col} (Anomalies: {len(anomalies)}, Normal: {len(normal)}) ---")

    if len(anomalies) == 0:
        print("No anomalies found for comparison.")
        return

    comparison_data = []
    for feature in feature_list:
        if feature not in df.columns:
            print(f"  Skipping '{feature}': Not found in DataFrame.")
            continue

        if compare_type == 'numerical':
            mean_anomaly = anomalies[feature].mean()
            median_anomaly = anomalies[feature].median()
            mean_normal = normal[feature].mean()
            median_normal = normal[feature].median()
            comparison_data.append({
                'Feature': feature,
                'Mean (Anomaly)': mean_anomaly,
                'Median (Anomaly)': median_anomaly,
                'Mean (Normal)': mean_normal,
                'Median (Normal)': median_normal
            })
        elif compare_type == 'binary_flag':
            # Calculate proportion of anomalies/normal where flag is 1
            prop_anomaly = anomalies[feature].mean() # For 0/1 flags, mean is proportion of 1s
            prop_normal = normal[feature].mean()
            comparison_data.append({
                'Feature': feature,
                'Prop=1 (Anomaly)': prop_anomaly,
                'Prop=1 (Normal)': prop_normal
            })
        elif compare_type == 'categorical':
             # Show top value counts for anomalies vs normal
             print(f"\n  Distribution for '{feature}':")
             print("    Anomalies (Top 3):")
             print(anomalies[feature].fillna('__NaN__').value_counts(normalize=True).head(3).to_string())
             print("    Normal (Top 3):")
             print(normal[feature].fillna('__NaN__').value_counts(normalize=True).head(3).to_string())


    if compare_type == 'numerical' or compare_type == 'binary_flag':
        comparison_df = pd.DataFrame(comparison_data)
        # Format numbers for better readability
        for col in comparison_df.columns:
            if comparison_df[col].dtype == 'float64':
                 comparison_df[col] = comparison_df[col].map('{:,.4f}'.format) # Adjust formatting as needed
        print(comparison_df.to_string(index=False))


# --- 1. Load Required Data ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_EVAL}] Loading Data")
df_engineered = None
df_results = None

# Load Original Engineered Features
try:
    print(f"Loading original engineered features from: {input_engineered_path}")
    df_engineered = pd.read_csv(input_engineered_path, low_memory=False)
    print(f"  Loaded df_engineered: {df_engineered.shape}")
except FileNotFoundError: print(f"CRITICAL ERROR: Engineered features file not found at {input_engineered_path}"); exit()
except Exception as e: print(f"CRITICAL ERROR loading engineered features: {e}"); traceback.print_exc(); exit()

# Load Baseline Model Results
try:
    print(f"Loading baseline results from: {input_results_path}")
    df_results = pd.read_csv(input_results_path, low_memory=False)
    print(f"  Loaded df_results: {df_results.shape}")
except FileNotFoundError: print(f"CRITICAL ERROR: Baseline results file not found at {input_results_path}"); exit()
except Exception as e: print(f"CRITICAL ERROR loading baseline results: {e}"); traceback.print_exc(); exit()

# --- 2. Merge Results with Features ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_EVAL}] Merging Results with Features")

# Check if 'original_index' exists in results, otherwise merge on identifier keys
merge_keys = identifier_cols = ['bukrs', 'belnr', 'gjahr', 'buzei'] # Define standard keys
if 'original_index' in df_results.columns:
    print("Merging on 'original_index'...")
    # Ensure original index exists in df_engineered if reset_index wasn't used before saving it
    if 'original_index' not in df_engineered.columns:
         df_engineered.reset_index(inplace=True) # Add index if missing
         df_engineered.rename(columns={'index':'original_index'}, inplace=True)
    df_eval = pd.merge(df_engineered, df_results, on='original_index', how='left', suffixes=('', '_res'))
    # Drop potentially duplicated identifier columns from results if needed
    id_cols_res = [f"{col}_res" for col in identifier_cols if f"{col}_res" in df_eval.columns]
    df_eval.drop(columns=id_cols_res, inplace=True)
else:
    print(f"Merging on keys: {merge_keys}...")
    # Ensure keys are string type in both dataframes before merge
    for col in merge_keys:
        if col in df_engineered.columns: df_engineered[col] = safe_to_string(df_engineered[col])
        if col in df_results.columns: df_results[col] = safe_to_string(df_results[col])
    df_eval = pd.merge(df_engineered, df_results, on=merge_keys, how='left')

print(f"Merged evaluation DataFrame shape: {df_eval.shape}")
# Verify merge success (check if label columns are present and have expected non-null count)
if 'IF_Label' in df_eval.columns:
    print(f"  IF_Label non-null count after merge: {df_eval['IF_Label'].notna().sum()}")
if 'LOF_Label' in df_eval.columns:
    print(f"  LOF_Label non-null count after merge: {df_eval['LOF_Label'].notna().sum()}")

# Identify feature columns present in the loaded df_engineered
numerical_features_eval = [
    'hsl', # Include original amount for context
    'FE_AbsoluteAmount', 'FE_LogAmount',
    'FE_UserPostingFrequency', 'FE_UserAvgLogAmount', 'FE_AmountDeviationFromUserMean',
    'FE_AccountPostingFrequency', 'FE_AccountAvgLogAmount', 'FE_AmountDeviationFromAccountMean',
    'FE_DocTypeFrequency', 'FE_TCodeFrequency'
]
binary_features_eval = [
    'FE_IsOutsideBusinessHours', 'FE_IsWeekend',
    'FE_IsRareTCodeForUser', 'FE_IsMissingCostCenterForExpense'
]
categorical_features_eval = [ # For value count comparison
     'blart', 'tcode', 'usnam_bkpf', 'racct', 'drcrk'
]

numerical_features_eval = [f for f in numerical_features_eval if f in df_eval.columns]
binary_features_eval = [f for f in binary_features_eval if f in df_eval.columns]
categorical_features_eval = [f for f in categorical_features_eval if f in df_eval.columns]


# --- 3. Anomaly Profiling ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_EVAL}] Profiling Anomalies")

# --- 3a. Isolation Forest Anomalies ---
if 'IF_Label' in df_eval.columns:
    compare_feature_stats(df_eval, 'IF_Label', numerical_features_eval, compare_type='numerical')
    compare_feature_stats(df_eval, 'IF_Label', binary_features_eval, compare_type='binary_flag')
    compare_feature_stats(df_eval, 'IF_Label', categorical_features_eval, compare_type='categorical')

    # Optionally save the anomaly subset
    if SAVE_ANOMALY_SUBSETS:
        if_anomalies = df_eval[df_eval['IF_Label'] == -1].copy()
        if_anomaly_path = os.path.join(output_directory, if_anomaly_filename)
        try:
            if_anomalies.to_csv(if_anomaly_path, index=False, encoding='utf-8')
            print(f"\nSuccessfully saved IF anomalies to: {if_anomaly_path}")
        except Exception as e:
            print(f"ERROR saving IF anomalies: {e}")
else:
    print("\nSkipping Isolation Forest profiling (IF_Label not found).")

# --- 3b. Local Outlier Factor Anomalies ---
if 'LOF_Label' in df_eval.columns:
    compare_feature_stats(df_eval, 'LOF_Label', numerical_features_eval, compare_type='numerical')
    compare_feature_stats(df_eval, 'LOF_Label', binary_features_eval, compare_type='binary_flag')
    compare_feature_stats(df_eval, 'LOF_Label', categorical_features_eval, compare_type='categorical')

    # Optionally save the anomaly subset
    if SAVE_ANOMALY_SUBSETS:
        lof_anomalies = df_eval[df_eval['LOF_Label'] == -1].copy()
        lof_anomaly_path = os.path.join(output_directory, lof_anomaly_filename)
        try:
            lof_anomalies.to_csv(lof_anomaly_path, index=False, encoding='utf-8')
            print(f"\nSuccessfully saved LOF anomalies to: {lof_anomaly_path}")
        except Exception as e:
            print(f"ERROR saving LOF anomalies: {e}")
else:
    print("\nSkipping Local Outlier Factor profiling (LOF_Label not found).")


# --- 4. Evaluation Summary (Qualitative based on Profiling) ---
print_separator(f"[Phase {PHASE_NUMBER} Step {STEP_EVAL}] Evaluation Summary")
print("Review the comparison tables above to understand the characteristics of flagged anomalies.")
print("Key things to look for:")
print(" - Numerical Features: Do anomalies have significantly higher/lower means or medians (e.g., Deviation features, LogAmount)?")
print(" - Binary Flags: Is the proportion of anomalies with a specific flag (e.g., IsWeekend, IsOutsideBusinessHours) much higher than normal points?")
print(" - Categorical Features: Do anomalies concentrate under specific users, TCodes, Doc Types, or Accounts compared to normal distribution?")
print("\nThis profiling helps determine if the models are identifying potentially interesting or suspicious patterns.")


print(f"\n--- Script phase3_step4b_baseline_eval.py ({TIMESTAMP}) Complete ---")
if SAVE_ANOMALY_SUBSETS:
    print("Outputs generated (if models ran successfully):")
    if 'IF_Label' in df_eval.columns: print(f" - {if_anomaly_filename}")
    if 'LOF_Label' in df_eval.columns: print(f" - {lof_anomaly_filename}")