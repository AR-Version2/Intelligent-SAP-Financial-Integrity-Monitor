# -*- coding: utf-8 -*-
"""SAP Capstone - EDA_step2_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iR_LRbBQldOwNpUw-9ecfG_JmR7_TJ-6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import traceback
import os
import datetime

# --- Configuration ---
# Input Files (Cleaned from Phase 1)
output_directory = r"C:\Users\anith\Downloads\" # Dir containing cleaned files & for saving output
bkpf_cleaned_filename = "bkpf_cleaned_after_step1_20250428_165811.csv"
faglflexa_cleaned_filename = "faglflexa_cleaned_after_step1_20250428_165811.csv"

bkpf_cleaned_file_path = os.path.join(output_directory, bkpf_cleaned_filename)
faglflexa_cleaned_file_path = os.path.join(output_directory, faglflexa_cleaned_filename)

# Output File
engineered_output_filename = "sap_engineered_features.csv"
engineered_output_path = os.path.join(output_directory, engineered_output_filename)

# --- Feature Engineering Parameters ---
# !! ADJUST THESE BASED ON YOUR ANALYSIS / DOMAIN KNOWLEDGE !!
BUSINESS_HOUR_START = 8
BUSINESS_HOUR_END = 18 # (Exclusive, so includes 17:xx)
# Placeholder - Define actual expense account ranges/prefixes for your Chart of Accounts
EXPENSE_ACCOUNT_PREFIXES = ['4', '6', '7'] # Example: Common SAP expense account starting numbers
# Threshold for considering a TCode 'rare' for a user (e.g., occurs less than 5 times)
RARE_TCODE_THRESHOLD = 5

# --- Constants & Mappings ---
FAGLFLEXA_KEY_MAP = {'rbukrs': 'bukrs', 'docnr': 'belnr', 'ryear': 'gjahr', 'docln': 'buzei'}
STD_KEY_NAMES = ['bukrs', 'belnr', 'gjahr']
PLACEHOLDER_VALUES = ['', '0000000000', '0', '/','__NaN__', None, '#', '000000']

# --- Plotting Configuration ---
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 7) # Default figure size
PLOT_TIMESTAMP = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
SAVE_PLOTS = True
PLOT_OUTPUT_DIR = os.path.join(output_directory, f"EDA_Plots_{PLOT_TIMESTAMP}")


# --- Helper Functions ---
def print_separator(title=""):
    print("\n" + "="*30 + f" {title} " + "="*30)

def safe_to_datetime(series, **kwargs):
    return pd.to_datetime(series, errors='coerce', **kwargs)

def safe_to_numeric(series, **kwargs):
    return pd.to_numeric(series, errors='coerce', **kwargs)

def safe_to_string(series):
    if series is None: return None
    if pd.api.types.is_numeric_dtype(series):
        try: return series.astype(float).astype('Int64').astype(str)
        except (ValueError, TypeError): return series.astype(str)
    elif not pd.api.types.is_string_dtype(series): return series.astype(str)
    return series

def save_plot(plt_obj, filename_base, output_dir):
    """Saves the current matplotlib plot with timestamp."""
    if SAVE_PLOTS:
        try:
            filename = f"{filename_base}.png" # Keep names simple now
            path = os.path.join(output_dir, filename)
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
                print(f"   * Created plot directory: {output_dir}")
            plt_obj.savefig(path, bbox_inches='tight', dpi=150)
            print(f"   * Plot saved: {filename}")
        except Exception as e:
            print(f"   * WARNING: Could not save plot {filename_base}. Error: {e}")
    plt_obj.close() # Close plot to free memory

# --- 1. Load Cleaned Data ---
print_separator("Loading Cleaned Data")
df_bkpf_clean = None; df_faglflexa_clean = None;
try:
    print(f"Loading BKPF from: {bkpf_cleaned_file_path}")
    df_bkpf_clean = pd.read_csv(bkpf_cleaned_file_path, low_memory=False)
    print(f"Loaded Cleaned BKPF: {df_bkpf_clean.shape}")
    print(f"Loading FAGLFLEXA from: {faglflexa_cleaned_file_path}")
    df_faglflexa_clean = pd.read_csv(faglflexa_cleaned_file_path, low_memory=False)
    print(f"Loaded Cleaned FAGLFLEXA: {df_faglflexa_clean.shape}")
except FileNotFoundError as fnf_error: print(f"CRITICAL ERROR: File not found: {fnf_error}"); exit()
except Exception as e: print(f"CRITICAL ERROR loading data: {e}"); traceback.print_exc(); exit()


# --- 2. Re-apply Type Conversions ---
print_separator("Applying Type Conversions")
# BKPF Conversions
key_cols = ['bukrs', 'belnr', 'gjahr']; date_cols = ['bldat', 'budat', 'cpudt']; num_cols = ['kursf']
str_cols = ['blart', 'monat', 'usnam', 'tcode', 'xblnr', 'bktxt', 'waers', 'hwaer', 'awtyp', 'awkey', 'cputm']
print("Converting BKPF types...")
for col in key_cols+date_cols+num_cols+str_cols:
    if col in df_bkpf_clean.columns:
        if col in key_cols+str_cols: df_bkpf_clean[col] = safe_to_string(df_bkpf_clean[col])
        elif col in date_cols: df_bkpf_clean[col] = safe_to_datetime(df_bkpf_clean[col]) # Allow format inference
        elif col in num_cols: df_bkpf_clean[col] = safe_to_numeric(df_bkpf_clean[col])
# FAGLFLEXA Conversions
key_cols = list(FAGLFLEXA_KEY_MAP.keys()); redundant_keys = ['belnr', 'buzei', 'gjahr']; date_cols = ['budat']; num_cols = ['hsl', 'tsl']
str_cols = ['rldnr', 'racct', 'drcrk', 'rtcur', 'rwcur', 'rcntr', 'prctr', 'segment', 'rfarea', 'rbusa', 'kokrs', 'poper', 'usnam', 'awtyp']
print("Converting FAGLFLEXA types...")
for col in key_cols + redundant_keys + date_cols + num_cols + str_cols:
     if col in df_faglflexa_clean.columns:
        if col in key_cols+redundant_keys+str_cols: df_faglflexa_clean[col] = safe_to_string(df_faglflexa_clean[col])
        elif col in date_cols: df_faglflexa_clean[col] = safe_to_datetime(df_faglflexa_clean[col])
        elif col in num_cols: df_faglflexa_clean[col] = safe_to_numeric(df_faglflexa_clean[col])

# --- 3. Merge Data ---
print_separator("Merging BKPF and FAGLFLEXA Data")
# Rename FAGLFLEXA keys to standard names for merging
df_faglflexa_renamed = df_faglflexa_clean.rename(columns=FAGLFLEXA_KEY_MAP)

# Perform the merge
print(f"Merging FAGLFLEXA ({df_faglflexa_renamed.shape}) with BKPF ({df_bkpf_clean.shape})")
# Ensure keys are compatible types (already done via safe_to_string)
merged_df = pd.merge(
    df_faglflexa_renamed,
    df_bkpf_clean,
    on=STD_KEY_NAMES, # ['bukrs', 'belnr', 'gjahr']
    how='left',       # Keep all FAGLFLEXA lines, add BKPF info
    suffixes=('_fagl', '_bkpf') # Suffixes for any remaining overlapping cols (e.g. budat)
)
print(f"Merged DataFrame shape: {merged_df.shape}")

# Verification: Check if any rows from FAGLFLEXA didn't find a BKPF match
bkpf_cols_to_check = [c for c in df_bkpf_clean.columns if c not in STD_KEY_NAMES]
null_bkpf_info = merged_df[bkpf_cols_to_check].isnull().all(axis=1).sum()
if null_bkpf_info > 0:
    print(f"WARNING: {null_bkpf_info} FAGLFLEXA rows had no matching BKPF header info after merge!")
    # This shouldn't happen if Phase 1 consistency check passed on cleaned data
else:
    print("OK: All FAGLFLEXA rows successfully merged with BKPF header info.")

# Clean up intermediate frames to save memory
del df_faglflexa_renamed
del df_bkpf_clean
del df_faglflexa_clean


# --- 4. Exploratory Data Analysis (EDA) ---
print_separator("Exploratory Data Analysis (EDA)")
print(f"Output plots will be saved to: {PLOT_OUTPUT_DIR}")
if SAVE_PLOTS and not os.path.exists(PLOT_OUTPUT_DIR):
    os.makedirs(PLOT_OUTPUT_DIR)

# --- 4.1 Transaction Volume & Value ---
print("\n--- 4.1 Volume & Value Analysis ---")
# Distribution of HSL (Amount)
plt.figure()
sns.histplot(merged_df['hsl'].dropna(), bins=100, kde=False)
plt.title('Distribution of HSL (Local Amount)')
plt.xlabel('HSL Amount'); plt.ylabel('Frequency'); plt.yscale('log')
save_plot(plt, "hsl_distribution", PLOT_OUTPUT_DIR)

plt.figure()
sns.boxplot(x=merged_df['hsl'].dropna())
plt.title('Box Plot of HSL (Local Amount)')
plt.xscale('symlog') # Use symlog for boxplot if data spans positive/negative significantly
save_plot(plt, "hsl_boxplot", PLOT_OUTPUT_DIR)
print("HSL Amount Description:\n", merged_df['hsl'].describe().apply(lambda x: f"{x:,.2f}"))

# Activity over time (Monthly)
if 'budat_bkpf' in merged_df.columns and pd.api.types.is_datetime64_any_dtype(merged_df['budat_bkpf']):
    print("\nAnalyzing activity over time...")
    # Ensure budat is datetime
    merged_df['budat_bkpf'] = pd.to_datetime(merged_df['budat_bkpf'], errors='coerce')
    # Drop rows where date conversion failed
    monthly_activity = merged_df.dropna(subset=['budat_bkpf']).copy()
    monthly_activity['post_month'] = monthly_activity['budat_bkpf'].dt.to_period('M')
    monthly_summary = monthly_activity.groupby('post_month').agg(
        doc_count=('belnr', 'nunique'), # Count unique documents per month
        line_item_count=('belnr', 'size'), # Count line items per month
        total_abs_hsl=('hsl', lambda x: x.abs().sum()) # Sum absolute value
    ).sort_index()

    print("Monthly Activity Summary (Sample):\n", monthly_summary.head())

    plt.figure()
    monthly_summary['line_item_count'].plot(kind='line', marker='o')
    plt.title('Monthly Line Item Volume')
    plt.ylabel('Number of Line Items')
    plt.xlabel('Posting Month')
    plt.xticks(rotation=45)
    plt.tight_layout()
    save_plot(plt, "monthly_volume", PLOT_OUTPUT_DIR)

    plt.figure()
    monthly_summary['total_abs_hsl'].plot(kind='line', marker='o', color='orange')
    plt.title('Monthly Total Absolute Value (HSL)')
    plt.ylabel('Total Absolute HSL')
    plt.xlabel('Posting Month')
    plt.xticks(rotation=45)
    plt.tight_layout()
    save_plot(plt, "monthly_value", PLOT_OUTPUT_DIR)
else:
    print("Skipping time series analysis - 'budat_bkpf' column missing or not datetime.")


# --- 4.2 Timing Analysis ---
print("\n--- 4.2 Timing Analysis ---")
# Distribution by Hour of Day (Using BKPF Posting Date/Time if available)
# Prefer CPUDT/CPUTM if reliable, else use BUDAT (no time info usually)
# Let's try CPUDT/CPUTM first
time_col_used = None
if 'cpudt' in merged_df.columns and 'cputm' in merged_df.columns:
     merged_df['entry_datetime'] = pd.to_datetime(
         merged_df['cpudt'].astype(str) + ' ' + merged_df['cputm'].astype(str),
         errors='coerce', format='%Y-%m-%d %H:%M:%S', exact=False # Adjust format if needed
     )
     if merged_df['entry_datetime'].notna().sum() > 0: # Check if conversion worked for some rows
         print("Using CPUDT/CPUTM for Posting Hour analysis.")
         merged_df['posting_hour'] = merged_df['entry_datetime'].dt.hour
         time_col_used = 'entry_datetime'
     else:
         print("CPUDT/CPUTM conversion failed, attempting BUDAT for day/month analysis.")
elif 'budat_bkpf' in merged_df.columns and pd.api.types.is_datetime64_any_dtype(merged_df['budat_bkpf']):
     print("Using BUDAT for Posting Day/Month analysis (Hour not available).")
     time_col_used = 'budat_bkpf'
else:
     print("Skipping detailed timing analysis - No suitable date/time column found (cpudt/cputm or budat_bkpf).")

if 'posting_hour' in merged_df.columns:
    plt.figure()
    sns.countplot(x='posting_hour', data=merged_df, palette='viridis')
    plt.title('Distribution of Postings by Hour of Day (Based on Entry Time)')
    plt.xlabel('Hour of Day (0-23)')
    plt.ylabel('Number of Line Items')
    save_plot(plt, "posting_hour_distribution", PLOT_OUTPUT_DIR)

# Distribution by Day of Week/Month (Using BUDAT)
if time_col_used == 'budat_bkpf' or time_col_used == 'entry_datetime': # Use entry_datetime if available, else budat
    date_for_day = merged_df[time_col_used]
    merged_df['posting_dayofweek'] = date_for_day.dt.dayofweek # Monday=0, Sunday=6
    merged_df['posting_dayofmonth'] = date_for_day.dt.day

    plt.figure()
    sns.countplot(x='posting_dayofweek', data=merged_df, palette='magma', order=range(7))
    plt.title('Distribution of Postings by Day of Week')
    plt.xlabel('Day of Week (0=Mon, 6=Sun)')
    plt.ylabel('Number of Line Items')
    plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
    save_plot(plt, "posting_dayofweek_distribution", PLOT_OUTPUT_DIR)

    plt.figure()
    sns.countplot(x='posting_dayofmonth', data=merged_df, palette='crest')
    plt.title('Distribution of Postings by Day of Month')
    plt.xlabel('Day of Month')
    plt.ylabel('Number of Line Items')
    save_plot(plt, "posting_dayofmonth_distribution", PLOT_OUTPUT_DIR)


# --- 4.3 User Activity ---
print("\n--- 4.3 User Activity Analysis ---")
if 'usnam_bkpf' in merged_df.columns:
    print("Top 10 Users by Line Item Count:")
    top_users = merged_df['usnam_bkpf'].value_counts().head(10)
    print(top_users)
    plt.figure(figsize=(12,6))
    sns.barplot(x=top_users.index, y=top_users.values, palette='rocket')
    plt.title('Top 10 Users by Line Item Posting Count')
    plt.ylabel('Number of Line Items')
    plt.xlabel('User Name')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    save_plot(plt, "top_users_by_count", PLOT_OUTPUT_DIR)

    # Box plot of HSL by Top Users
    plt.figure(figsize=(14, 7))
    # Filter for top users to make the plot readable
    top_user_list = top_users.index.tolist()
    sns.boxplot(x='usnam_bkpf', y='hsl', data=merged_df[merged_df['usnam_bkpf'].isin(top_user_list)], palette='Spectral')
    plt.title('Distribution of HSL Amount by Top 10 Users')
    plt.xlabel('User Name')
    plt.ylabel('HSL Amount')
    plt.yscale('symlog') # Use symlog if amounts vary greatly
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    save_plot(plt, "hsl_boxplot_by_user", PLOT_OUTPUT_DIR)
else:
    print("Skipping User Activity analysis - 'usnam_bkpf' column not found.")


# --- 4.4 Process Context ---
print("\n--- 4.4 Process Context Analysis ---")
if 'blart' in merged_df.columns:
    print("Top 10 Document Types (blart) by Line Item Count:")
    top_blart = merged_df['blart'].value_counts().head(10)
    print(top_blart)
    plt.figure(figsize=(12,6)); sns.barplot(x=top_blart.index, y=top_blart.values, palette='coolwarm')
    plt.title('Top 10 Document Types (blart)'); plt.ylabel('Line Item Count'); plt.xlabel('Document Type')
    plt.tight_layout(); save_plot(plt, "top_document_types", PLOT_OUTPUT_DIR)

if 'tcode' in merged_df.columns:
    print("\nTop 10 Transaction Codes (tcode) by Line Item Count:")
    top_tcode = merged_df['tcode'].value_counts().head(10)
    print(top_tcode)
    plt.figure(figsize=(12,6)); sns.barplot(x=top_tcode.index, y=top_tcode.values, palette='viridis')
    plt.title('Top 10 Transaction Codes (tcode)'); plt.ylabel('Line Item Count'); plt.xlabel('Transaction Code')
    plt.xticks(rotation=45, ha='right'); plt.tight_layout(); save_plot(plt, "top_tcodes", PLOT_OUTPUT_DIR)

# Relationship between blart and amount ranges (Top 5 blart)
if 'blart' in merged_df.columns and 'hsl' in merged_df.columns:
     top_blart_list = merged_df['blart'].value_counts().head(5).index.tolist()
     plt.figure(figsize=(14, 7))
     sns.boxplot(x='blart', y='hsl', data=merged_df[merged_df['blart'].isin(top_blart_list)], palette='Pastel1')
     plt.title('Distribution of HSL Amount by Top 5 Document Types')
     plt.xlabel('Document Type (blart)'); plt.ylabel('HSL Amount'); plt.yscale('symlog')
     plt.tight_layout(); save_plot(plt, "hsl_boxplot_by_blart", PLOT_OUTPUT_DIR)


# --- 4.5 Account & Dimension Analysis ---
print("\n--- 4.5 Account & Dimension Analysis ---")
if 'racct' in merged_df.columns:
    print("Top 10 GL Accounts (racct) by Line Item Count:")
    top_racct = merged_df['racct'].value_counts().head(10)
    print(top_racct)
    plt.figure(figsize=(12,6)); sns.barplot(x=top_racct.index, y=top_racct.values, palette='cubehelix')
    plt.title('Top 10 GL Accounts (racct)'); plt.ylabel('Line Item Count'); plt.xlabel('GL Account')
    plt.xticks(rotation=45, ha='right'); plt.tight_layout(); save_plot(plt, "top_gl_accounts", PLOT_OUTPUT_DIR)

# Check population again just to be sure (might show slightly different % if merge dropped rows, unlikely here)
if 'rcntr' in merged_df.columns: print(f"Cost Center (rcntr) Population in Merged: {merged_df['rcntr'].notna().mean():.2%}")
if 'prctr' in merged_df.columns: print(f"Profit Center (prctr) Population in Merged: {merged_df['prctr'].notna().mean():.2%}")
if 'segment' in merged_df.columns: print(f"Segment Population in Merged: {merged_df['segment'].notna().mean():.2%}")


# --- 5. Feature Engineering ---
print_separator("Feature Engineering")

# --- 5.1 Timing Features ---
print("Creating timing features...")
if 'posting_hour' in merged_df.columns:
    merged_df['FE_PostingHour'] = merged_df['posting_hour']
    merged_df['FE_IsOutsideBusinessHours'] = np.where(
        (merged_df['posting_hour'] < BUSINESS_HOUR_START) | (merged_df['posting_hour'] >= BUSINESS_HOUR_END), 1, 0
    )
    print(f"  Created FE_PostingHour, FE_IsOutsideBusinessHours (Business Hours: {BUSINESS_HOUR_START}-{BUSINESS_HOUR_END})")
else:
    print("  Skipping FE_PostingHour, FE_IsOutsideBusinessHours (posting_hour unavailable)")

if 'posting_dayofweek' in merged_df.columns:
    merged_df['FE_PostingDayOfWeek'] = merged_df['posting_dayofweek']
    merged_df['FE_IsWeekend'] = np.where(merged_df['posting_dayofweek'].isin([5, 6]), 1, 0) # 5=Sat, 6=Sun
    print("  Created FE_PostingDayOfWeek, FE_IsWeekend")
else:
     print("  Skipping FE_PostingDayOfWeek, FE_IsWeekend (posting_dayofweek unavailable)")

# --- 5.2 Magnitude Features ---
print("Creating magnitude features...")
if 'hsl' in merged_df.columns:
    merged_df['FE_AbsoluteAmount'] = merged_df['hsl'].abs()
    merged_df['FE_LogAmount'] = np.log1p(merged_df['FE_AbsoluteAmount']) # log1p handles 0 safely
    print("  Created FE_AbsoluteAmount, FE_LogAmount")
else:
     print("  Skipping magnitude features (hsl unavailable)")

# --- 5.3 User-Based Features ---
print("Creating user-based features...")
if 'usnam_bkpf' in merged_df.columns and 'FE_LogAmount' in merged_df.columns:
    # User Frequency
    user_counts = merged_df['usnam_bkpf'].map(merged_df['usnam_bkpf'].value_counts())
    merged_df['FE_UserPostingFrequency'] = user_counts
    print("  Created FE_UserPostingFrequency")

    # Amount Deviation from User Mean
    user_mean_log_amount = merged_df.groupby('usnam_bkpf')['FE_LogAmount'].transform('mean')
    merged_df['FE_UserAvgLogAmount'] = user_mean_log_amount
    merged_df['FE_AmountDeviationFromUserMean'] = merged_df['FE_LogAmount'] - merged_df['FE_UserAvgLogAmount']
    # Handle cases where user has only one posting (mean is the value itself, deviation is 0) - transform handles this implicitly.
    # Fill NaN deviations that might occur if LogAmount was NaN
    merged_df['FE_AmountDeviationFromUserMean'].fillna(0, inplace=True)
    print("  Created FE_UserAvgLogAmount, FE_AmountDeviationFromUserMean")

    # Rare TCode for User
    if 'tcode' in merged_df.columns:
        user_tcode_counts = merged_df.groupby(['usnam_bkpf', 'tcode'])['belnr'].transform('size')
        merged_df['FE_IsRareTCodeForUser'] = np.where(user_tcode_counts < RARE_TCODE_THRESHOLD, 1, 0)
        print(f"  Created FE_IsRareTCodeForUser (Threshold < {RARE_TCODE_THRESHOLD})")
    else:
         print("  Skipping FE_IsRareTCodeForUser (tcode unavailable)")
else:
     print("  Skipping user-based features ('usnam_bkpf' or 'FE_LogAmount' unavailable)")


# --- 5.4 Account/Dimension-Based Features ---
print("Creating account/dimension-based features...")
if 'racct' in merged_df.columns and 'FE_LogAmount' in merged_df.columns:
    # Account Frequency
    account_counts = merged_df['racct'].map(merged_df['racct'].value_counts())
    merged_df['FE_AccountPostingFrequency'] = account_counts
    print("  Created FE_AccountPostingFrequency")

    # Amount Deviation from Account Mean
    account_mean_log_amount = merged_df.groupby('racct')['FE_LogAmount'].transform('mean')
    merged_df['FE_AccountAvgLogAmount'] = account_mean_log_amount
    merged_df['FE_AmountDeviationFromAccountMean'] = merged_df['FE_LogAmount'] - merged_df['FE_AccountAvgLogAmount']
    merged_df['FE_AmountDeviationFromAccountMean'].fillna(0, inplace=True)
    print("  Created FE_AccountAvgLogAmount, FE_AmountDeviationFromAccountMean")

    # Missing Cost Center for Expense Accounts
    if 'rcntr' in merged_df.columns:
        print(f"  Checking for missing Cost Center for Expense Accounts (prefixes: {EXPENSE_ACCOUNT_PREFIXES})...")
        # Identify expense accounts (adjust logic based on your CoA)
        is_expense = merged_df['racct'].astype(str).str.startswith(tuple(EXPENSE_ACCOUNT_PREFIXES))
        # Identify missing/placeholder cost centers
        is_missing_rcntr = merged_df['rcntr'].isnull() | merged_df['rcntr'].astype(str).str.strip().isin(PLACEHOLDER_VALUES)
        merged_df['FE_IsMissingCostCenterForExpense'] = np.where(is_expense & is_missing_rcntr, 1, 0)
        print(f"  Created FE_IsMissingCostCenterForExpense ({merged_df['FE_IsMissingCostCenterForExpense'].sum()} instances found)")
    else:
         print("  Skipping FE_IsMissingCostCenterForExpense (rcntr unavailable)")
else:
     print("  Skipping account-based features ('racct' or 'FE_LogAmount' unavailable)")

# --- 5.5 Contextual Features ---
print("Creating contextual features...")
if 'blart' in merged_df.columns:
     merged_df['FE_DocTypeFrequency'] = merged_df['blart'].map(merged_df['blart'].value_counts())
     print("  Created FE_DocTypeFrequency")
if 'tcode' in merged_df.columns:
     merged_df['FE_TCodeFrequency'] = merged_df['tcode'].map(merged_df['tcode'].value_counts())
     print("  Created FE_TCodeFrequency")


# --- 6. Final Dataset Preparation ---
print_separator("Preparing Final Engineered Dataset")

# Select engineered features + key identifiers + potentially useful original fields
feature_cols = [col for col in merged_df.columns if col.startswith('FE_')]
# Add key fields needed for identification/joining later if needed
# Add core fields that might still be useful raw
identifier_cols = ['bukrs', 'belnr', 'gjahr', 'buzei'] # Standard keys
original_fields_to_keep = [
    'racct', 'hsl', 'drcrk', # Core FAGL item
    'blart', 'budat_bkpf', 'usnam_bkpf', 'tcode', # Core BKPF context
    'rcntr', 'prctr', 'segment' # Key Dimensions
]
# Ensure columns actually exist in merged_df
final_cols = identifier_cols + \
             [col for col in original_fields_to_keep if col in merged_df.columns] + \
             feature_cols

# Create final DataFrame
df_engineered = merged_df[final_cols].copy()

# Final check for NaNs introduced during feature engineering
nan_check = df_engineered[feature_cols].isnull().sum()
print("\nNaN check on Engineered Features:")
print(nan_check[nan_check > 0])
if nan_check.sum() > 0:
     print("WARNING: NaNs found in engineered features. Consider imputation if necessary for modeling.")
     # Simple imputation example (fill with 0, but choose strategy carefully):
     # for col in feature_cols:
     #     if df_engineered[col].isnull().any():
     #         print(f"  Imputing NaNs in {col} with 0.")
     #         df_engineered[col].fillna(0, inplace=True)

print(f"Final Engineered DataFrame shape: {df_engineered.shape}")
print("Final Engineered DataFrame Columns:\n", df_engineered.columns.tolist())
print("\nSample Engineered Data (10 rows):")
print(df_engineered.sample(n=min(10, len(df_engineered)), random_state=42))


# --- 7. Export Engineered Dataset ---
print_separator("Exporting Engineered Dataset")
try:
    df_engineered.to_csv(engineered_output_path, index=False, encoding='utf-8')
    print(f"Successfully exported engineered data to: {engineered_output_path}")
except Exception as e:
    print(f"ERROR: Failed to export engineered data: {e}")
    traceback.print_exc()


print("\nPhase 2: EDA and Feature Engineering Complete.")